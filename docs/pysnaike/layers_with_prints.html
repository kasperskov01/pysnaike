<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pysnaike.layers_with_prints API documentation</title>
<meta name="description" content="Different types of layers in a network." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pysnaike.layers_with_prints</code></h1>
</header>
<section id="section-intro">
<p>Different types of layers in a network.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Different types of layers in a network.&#34;&#34;&#34;


import numpy as np
np.random.seed(2)
from pysnaike import activations
import sys


class Dense():
    def __init__(self, size: int, name: str = &#34;dense&#34;, activation=activations.leaky_relu):
        &#34;&#34;&#34;Dense layer.

        Args:
            size (int): Number of perceptrons in the layer.
            name (str, optional): Name of the input layer.
            activation (optional): Activation function used at this layer.
        &#34;&#34;&#34;

        self.size = size
        self.name = name
        self.activation = activation

        self.num_params = self.size * 2
        self.output_shape = self.size
        self.layer_idx = None


    def setup(self, **kvargs):
        &#34;&#34;&#34;Called on model compile.

        Returns:
            (w, b): Weight and bias.
        &#34;&#34;&#34;

        self.layer_idx = kvargs[&#39;layer_idx&#39;]
        w = np.random.randn(kvargs[&#39;size_curr&#39;], kvargs[&#39;size_prev&#39;]) * 0.1
        b = np.zeros(kvargs[&#39;size_curr&#39;])
        return w, b

    def forward_pass(self, params):
        &#34;&#34;&#34;Forward pass through dense layer.

        Args:
            params (dict): Model parameters.
        &#34;&#34;&#34;
        w = params[f&#39;W{self.layer_idx}&#39;]
        b = params[f&#39;B{self.layer_idx}&#39;]
        prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]

        # Referenced object `params` is modified
        params[f&#39;Z{self.layer_idx}&#39;] = np.dot(w, prev_a) + b
        params[f&#39;A{self.layer_idx}&#39;] = self.activation(params[f&#39;Z{self.layer_idx}&#39;])

    def backward_pass(self, error, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):

        if not is_last:
            error = np.dot(model.params[&#39;W&#39; + str(self.layer_idx + 1)].T, error) * model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)
        else:
            error = 2 * (output - target) / output.shape[0] * model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)

        new_params[&#39;W&#39; + str(self.layer_idx)] = np.outer(error, model.params[&#39;A&#39; + str(self.layer_idx - 1)])
        new_params[&#39;B&#39; + str(self.layer_idx)] = error
        return error


    def __str__(self) -&gt; str:
        return f&#34;&lt;Dense layer class object named &#39;{self.name}&#39; of size {self.size} using activation function &#39;{self.activation.__name__}&#39;&gt;&#34;


class Conv2D():
    def __init__(self, num_filters, kernel_size=(3, 3), input_shape=(1, 5, 5), strides=(1,1), kernel=None, padding=&#34;same&#34;, activation=activations.identity, name: str = &#34;conv2d&#34;):
        &#34;&#34;&#34;2D convolutional layer.

        Not sure activations are working properly.

        Args:
            num_filters (int): Number of filters in layer.
            kernel_size ((x, y), optional): Size of kernel Both dimensions must be odd. Defaults to (3, 3).
            input_shape ((channels, x, y), optional): Shape of input data. Defaults to (2, 5, 5).
            strides ((x, y), optional): NOT implemented. Step size when sliding across the grid. Defaults to (1, 1).
            padding (optional): Either &#34;same&#34;, &#34;valid&#34; or an integer. &#34;same&#34; means padding is added evenly on all sides, so that output shape equals input shape. &#34;valid&#34; is the same as no padding. Defaults to &#34;same&#34;.
            activation (optional): Activation function used at this layer.
            name (str, optional): Name of the layer. Defaults to &#34;conv2d&#34;.

        Todo:
            Stride
        &#34;&#34;&#34;

        self.num_filters = num_filters
        self.kernel_size = np.array(kernel_size)
        self.input_shape = np.array(input_shape)
        self.strides = np.array(strides)
        self.kernel = kernel
        self.padding = self.calc_padding(padding)
        self.activation = activation
        self.name = name

        self.layer_idx = 0 # Changed at model compile
        self.num_params = self.calc_num_params()
        self.output_shape = self.calc_output_shape()

    def setup(self, **kvargs):
        &#34;&#34;&#34;Setup layer and instantiate weights and biases.

        Returns:
            list: Weights and biases.
        &#34;&#34;&#34;

        self.layer_idx = kvargs[&#39;layer_idx&#39;]
        
        w = self.kernel
        if self.kernel is None:
            w = np.random.randn(self.num_filters, self.kernel_size[0], self.kernel_size[1])

        b = np.zeros(self.num_filters)        
        return w, b

    def calc_padding(self, padding):
        &#34;&#34;&#34;Calculate padding size left and right in x-direction and up and down in y-direction.

        Args:
            padding: Either &#34;same&#34; or &#34;valid&#34;. &#34;same&#34;: Padding is added equally on all sides and output grid has same size as input. &#34;valid&#34;: No padding is added.

        Returns:
            list: Padding in x and y-direction.
        &#34;&#34;&#34;

        pad = np.array([0,0])
        if padding == &#39;same&#39;: pad = (self.kernel_size - 1) // 2
        return pad

    def calc_num_params(self):
        &#34;&#34;&#34;Calculate number of tweakable weights and biases in layer.

        Output as calculated with the formula:
        Filter size prod (m x n) * num channels * num filters + bias (which is num filters)

        Returns:
            int: Number of parameters.
        &#34;&#34;&#34;

        return np.prod(self.kernel_size) * self.input_shape[0] * self.num_filters + self.num_filters

    def calc_output_shape(self):
        &#34;&#34;&#34;Output shape of convolutional layer.

        Returns:
            [out_x, out_y]: Numpy array with layer output size x and output size y.
        &#34;&#34;&#34;
        return np.append(self.num_filters, [(self.input_shape[1:] + self.padding * 2 - self.kernel_size) // self.strides + 1])


    def forward_pass(self, params):
        &#34;&#34;&#34;Perform forward pass through layer based on weights and biases from `params`. 
        Model `params` is updated in place.

        Args:
            params (dict): Model parameters for all layers.
        &#34;&#34;&#34;
        w = params[f&#39;W{self.layer_idx}&#39;]
        b = params[f&#39;B{self.layer_idx}&#39;]
        prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]
        
        conv = self.conv(prev_a, w, self.padding)
        params[f&#39;Z{self.layer_idx}&#39;] = (conv / np.prod(self.kernel_size)) + b[:, np.newaxis, np.newaxis]
        params[f&#39;A{self.layer_idx}&#39;] = self.activation(params[f&#39;Z{self.layer_idx}&#39;])

    def conv(self, a, b, padding, sum_out=False):
        &#34;&#34;&#34;Full convolution between matrices `a` and `b`.

        Args:
            a: Stationary matrix.
            b: Moving matrix.
            padding: Two numbers representing padding

        Returns:
            array: Matrix containing convoluted output.
        &#34;&#34;&#34;
        print(&#34;a padd&#34;)                
        a_with_pad = np.zeros((a.shape[0], *(a.shape[2:] + padding * 2)))
        print(a_with_pad.shape)
        print(a_with_pad)

        a_with_pad[:, padding[0] : padding[0] + a.shape[1], padding[1]:padding[1] + a.shape[2]] = a
        print(&#39;full conv a_with_pad&#39;)
        print(a_with_pad)

        # print((b.shape[0], *(a.shape[1:] + padding * 2 - b.shape[1:] + 1)))
        out = np.zeros((b.shape[0], *(a.shape[1:] + padding * 2 - b.shape[1:] + 1)))
        print(&#34;out&#34;)
        print(out.shape)
        print(out)

        print(&#34;a matrix&#34;)
        print(a.round(2))
        print(&#34;b matrix&#34;)
        print(b.round(2))
        for x in range(out.shape[1]):
            for y in range(out.shape[2]):
                # print(f&#34;x,y: {x,y}&#34;)                
                # print(a_with_pad.shape)
                # print(a.shape)
                # print(&#34;not moving&#34;)
                # print(a_with_pad[:, x:x + b.shape[1], y:y + b.shape[2]].flatten())
                # print(&#34;moving&#34;)
                # print(b[:].flatten())
                # print(&#34;tensordot&#34;)
                # print((a_with_pad[:, x:x + b.shape[1], y:y + b.shape[2]]).shape)
                # print(b[:].shape)                
                dot = np.tensordot(a_with_pad[:, x:x + b.shape[1], y:y + b.shape[2]], b, axes=([1,2], [1,2]))
                # dot = np.dot(a_with_pad[:, x:x + b.shape[1], y:y + b.shape[2]].flatten(), b[:].flatten())
                # print(&#34;dot.shape&#34;)                
                # print(dot)
                # print(out.shape)
                # print(dot)
                # print(f&#34;{x, y} full conv saving&#34;)
                # print(out)
                out[:,x,y] = dot
                # print(out)
        # print(&#34;Done full convolution&#34;)
        # print(&#34;a_with_pad&#34;)
        # print(a_with_pad)
        # print(&#34;b&#34;)
        # print(b)
        # print(&#34;out&#34;)
        # print(out)
        print(&#34;out not summed&#34;)
        print(out.round(2))
        if sum_out:
            out = np.sum(out, axis=0)
        return out


    def backward_pass(self, error, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
        print(&#39;output after activation function&#39;)
        print(output.round(2))
        print(&#39;target&#39;)
        print(target.round(2))
        print(&#39;2*(output-target)&#39;)
        print((2*(output-target)).round(2))
        print(f&#39;layer activation {model.layers[self.layer_idx].activation}&#39;)
        print(&#39;Z deriv true&#39;)
        print(model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True))

        if not is_last:
            print(&#34;Not last&#34;)
            print(error.round(2))
            print(error.shape)
            print(&#34;W&#34;)
            w = model.params[&#39;W&#39; + str(self.layer_idx)]
            print(w.round(2))
            print(&#34;180 rotate (two flips):&#34;)
            arr = np.flip(np.flip(w, -2), -1)
            print(arr.round(2))
            print(arr.shape)

            if len(arr.shape) &gt; 3:
                print(&#34;Swap axes:&#34;)
                arr = np.swapaxes(arr, 0,1)
                print(arr.round(2))
                print(arr.shape)
            else: print(f&#34;Not swapping axes. Only {len(arr.shape)}&#34;)

            print(&#34;w&#34;)
            print(w.round(2))
            print(&#34;padding&#34;)
            padding = np.array(error.shape[-2:]) - 1
            print(padding.round(2))
            print(&#34;Full conv&#34;)

            print(w.shape)
            print(&#34;arr&#34;)
            print(arr.round(2))
            print(&#34;error&#34;)
            print(error.shape)
            print(error.round(2))            
            print(&#34;padding&#34;)
            print(padding.round(2))
            error = self.conv(arr, error, padding)

            pass
            # error = np.dot(model.params[&#39;W&#39; + str(self.layer_idx + 1)].T, error) * model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)


        else:
            error = 2 * (output - target) * model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)
            print(&#39;(error) dL/d omega&#39;)
            print(error.round(2))

        # new_params[&#39;W&#39; + str(self.layer_idx)] = np.outer(error, model.params[&#39;A&#39; + str(self.layer_idx - 1)])
        prev_a = model.params[&#39;A&#39; + str(self.layer_idx - 1)]
        print(&#39;prev_a&#39;)
        print(prev_a.round(2))
        out_shape = model.params[&#39;W&#39; + str(self.layer_idx)].shape
        print(&#39;convolve back&#39;)
        print(out_shape)
        print(error.shape)
        print(prev_a.shape)
        print(self.padding)
        print(&#34;done printing&#34;)
        # print(&#34;Using full conv&#34;)
        # print(self.conv(prev_a, error, self.padding))        
        new_w = self.conv(prev_a, error, self.padding)
        print(&#34;new_w&#34;)
        print(new_w)
        new_params[&#39;W&#39; + str(self.layer_idx)] = new_w / np.prod(new_w.shape[1:]) * model.learning_rate
        print(new_w / np.prod(new_w.shape[1:]) * model.learning_rate)
        print(&#39;W edits&#39;)
        print(new_params[&#39;W&#39; + str(self.layer_idx)].round(6))

        print(&#34;B (error)&#34;)
        print(error.round(2))
        print(&#34;B&#34;)
        print(np.sum(error, axis=(1,2)).round(2))
        # Bias should work with this line, but it makes things worse somehow
        # new_params[f&#39;B{self.layer_idx}&#39;] = np.sum(error, axis=(1,2)) / np.prod(error.shape[1:]) * model.learning_rate
        new_params[f&#39;B{self.layer_idx}&#39;] = np.zeros(self.num_filters)

        print(f&#34;new_params[B{self.layer_idx}]&#34;)
        print(new_params[&#39;B&#39; + str(self.layer_idx)].round(2))
        return error


    def __str__(self) -&gt; str:
        return f&#34;&lt;Conv2D layer class object named &#39;{self.name}&#39; with input shape {self.input_shape} and filter shape &#39;{self.kernel_size}&#39;&gt;&#34;


class MaxPooling2D():
    def __init__(self, pool_size):
        pass


class AvgPooling2D():
    def __init__(self, pool_size):
        pass


class Flatten():
    def __init__(self, name=&#39;flatten&#39;):
        self.name = name
        self.output_shape = None
        self.num_params = None

    def setup(self, **kvargs):
        self.layer_idx = kvargs[&#39;layer_idx&#39;]

        if kvargs[&#39;size_prev&#39;].shape is not ():
            self.output_shape = np.prod(kvargs[&#39;size_prev&#39;][0:-1])
        else: self.output_shape = kvargs[&#39;size_prev&#39;]
        return None, None

    def forward_pass(self, params):
        pass
        # params[f&#39;Z{self.layer_idx}&#39;] = params[f&#39;Z{self.layer_idx - 1}&#39;]
        # params[f&#39;A{self.layer_idx}&#39;] = params[f&#39;A{self.layer_idx - 1}&#39;]





class Reshape():
    def __init__(self, output_shape, name=&#39;reshape&#39;):
        self.name = name
        self.output_shape = np.array(output_shape)
        self.num_params = None
        self.layer_idx = None
        self.activation = None

    def setup(self, **kvargs):
        self.layer_idx = kvargs[&#39;layer_idx&#39;]
        assert np.prod(kvargs[&#39;size_prev&#39;]) == np.prod(self.output_shape), f&#39;Reshape not possible from {kvargs[&#34;size_prev&#34;]} to {self.output_shape}&#39;

        return None, None

    def forward_pass(self, params):
        params[f&#39;Z{self.layer_idx}&#39;] = None
        params[f&#39;A{self.layer_idx}&#39;] = None</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pysnaike.layers_with_prints.AvgPooling2D"><code class="flex name class">
<span>class <span class="ident">AvgPooling2D</span></span>
<span>(</span><span>pool_size)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AvgPooling2D():
    def __init__(self, pool_size):
        pass</code></pre>
</details>
</dd>
<dt id="pysnaike.layers_with_prints.Conv2D"><code class="flex name class">
<span>class <span class="ident">Conv2D</span></span>
<span>(</span><span>num_filters, kernel_size=(3, 3), input_shape=(1, 5, 5), strides=(1, 1), kernel=None, padding='same', activation=&lt;function identity&gt;, name: str = 'conv2d')</span>
</code></dt>
<dd>
<div class="desc"><p>2D convolutional layer.</p>
<p>Not sure activations are working properly.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_filters</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of filters in layer.</dd>
<dt>kernel_size ((x, y), optional): Size of kernel Both dimensions must be odd. Defaults to (3, 3).</dt>
<dt>input_shape ((channels, x, y), optional): Shape of input data. Defaults to (2, 5, 5).</dt>
<dt>strides ((x, y), optional): NOT implemented. Step size when sliding across the grid. Defaults to (1, 1).</dt>
<dt><strong><code>padding</code></strong> :&ensp;<code>optional</code></dt>
<dd>Either "same", "valid" or an integer. "same" means padding is added evenly on all sides, so that output shape equals input shape. "valid" is the same as no padding. Defaults to "same".</dd>
<dt><strong><code>activation</code></strong> :&ensp;<code>optional</code></dt>
<dd>Activation function used at this layer.</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of the layer. Defaults to "conv2d".</dd>
</dl>
<h2 id="todo">Todo</h2>
<p>Stride</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Conv2D():
    def __init__(self, num_filters, kernel_size=(3, 3), input_shape=(1, 5, 5), strides=(1,1), kernel=None, padding=&#34;same&#34;, activation=activations.identity, name: str = &#34;conv2d&#34;):
        &#34;&#34;&#34;2D convolutional layer.

        Not sure activations are working properly.

        Args:
            num_filters (int): Number of filters in layer.
            kernel_size ((x, y), optional): Size of kernel Both dimensions must be odd. Defaults to (3, 3).
            input_shape ((channels, x, y), optional): Shape of input data. Defaults to (2, 5, 5).
            strides ((x, y), optional): NOT implemented. Step size when sliding across the grid. Defaults to (1, 1).
            padding (optional): Either &#34;same&#34;, &#34;valid&#34; or an integer. &#34;same&#34; means padding is added evenly on all sides, so that output shape equals input shape. &#34;valid&#34; is the same as no padding. Defaults to &#34;same&#34;.
            activation (optional): Activation function used at this layer.
            name (str, optional): Name of the layer. Defaults to &#34;conv2d&#34;.

        Todo:
            Stride
        &#34;&#34;&#34;

        self.num_filters = num_filters
        self.kernel_size = np.array(kernel_size)
        self.input_shape = np.array(input_shape)
        self.strides = np.array(strides)
        self.kernel = kernel
        self.padding = self.calc_padding(padding)
        self.activation = activation
        self.name = name

        self.layer_idx = 0 # Changed at model compile
        self.num_params = self.calc_num_params()
        self.output_shape = self.calc_output_shape()

    def setup(self, **kvargs):
        &#34;&#34;&#34;Setup layer and instantiate weights and biases.

        Returns:
            list: Weights and biases.
        &#34;&#34;&#34;

        self.layer_idx = kvargs[&#39;layer_idx&#39;]
        
        w = self.kernel
        if self.kernel is None:
            w = np.random.randn(self.num_filters, self.kernel_size[0], self.kernel_size[1])

        b = np.zeros(self.num_filters)        
        return w, b

    def calc_padding(self, padding):
        &#34;&#34;&#34;Calculate padding size left and right in x-direction and up and down in y-direction.

        Args:
            padding: Either &#34;same&#34; or &#34;valid&#34;. &#34;same&#34;: Padding is added equally on all sides and output grid has same size as input. &#34;valid&#34;: No padding is added.

        Returns:
            list: Padding in x and y-direction.
        &#34;&#34;&#34;

        pad = np.array([0,0])
        if padding == &#39;same&#39;: pad = (self.kernel_size - 1) // 2
        return pad

    def calc_num_params(self):
        &#34;&#34;&#34;Calculate number of tweakable weights and biases in layer.

        Output as calculated with the formula:
        Filter size prod (m x n) * num channels * num filters + bias (which is num filters)

        Returns:
            int: Number of parameters.
        &#34;&#34;&#34;

        return np.prod(self.kernel_size) * self.input_shape[0] * self.num_filters + self.num_filters

    def calc_output_shape(self):
        &#34;&#34;&#34;Output shape of convolutional layer.

        Returns:
            [out_x, out_y]: Numpy array with layer output size x and output size y.
        &#34;&#34;&#34;
        return np.append(self.num_filters, [(self.input_shape[1:] + self.padding * 2 - self.kernel_size) // self.strides + 1])


    def forward_pass(self, params):
        &#34;&#34;&#34;Perform forward pass through layer based on weights and biases from `params`. 
        Model `params` is updated in place.

        Args:
            params (dict): Model parameters for all layers.
        &#34;&#34;&#34;
        w = params[f&#39;W{self.layer_idx}&#39;]
        b = params[f&#39;B{self.layer_idx}&#39;]
        prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]
        
        conv = self.conv(prev_a, w, self.padding)
        params[f&#39;Z{self.layer_idx}&#39;] = (conv / np.prod(self.kernel_size)) + b[:, np.newaxis, np.newaxis]
        params[f&#39;A{self.layer_idx}&#39;] = self.activation(params[f&#39;Z{self.layer_idx}&#39;])

    def conv(self, a, b, padding, sum_out=False):
        &#34;&#34;&#34;Full convolution between matrices `a` and `b`.

        Args:
            a: Stationary matrix.
            b: Moving matrix.
            padding: Two numbers representing padding

        Returns:
            array: Matrix containing convoluted output.
        &#34;&#34;&#34;
        print(&#34;a padd&#34;)                
        a_with_pad = np.zeros((a.shape[0], *(a.shape[2:] + padding * 2)))
        print(a_with_pad.shape)
        print(a_with_pad)

        a_with_pad[:, padding[0] : padding[0] + a.shape[1], padding[1]:padding[1] + a.shape[2]] = a
        print(&#39;full conv a_with_pad&#39;)
        print(a_with_pad)

        # print((b.shape[0], *(a.shape[1:] + padding * 2 - b.shape[1:] + 1)))
        out = np.zeros((b.shape[0], *(a.shape[1:] + padding * 2 - b.shape[1:] + 1)))
        print(&#34;out&#34;)
        print(out.shape)
        print(out)

        print(&#34;a matrix&#34;)
        print(a.round(2))
        print(&#34;b matrix&#34;)
        print(b.round(2))
        for x in range(out.shape[1]):
            for y in range(out.shape[2]):
                # print(f&#34;x,y: {x,y}&#34;)                
                # print(a_with_pad.shape)
                # print(a.shape)
                # print(&#34;not moving&#34;)
                # print(a_with_pad[:, x:x + b.shape[1], y:y + b.shape[2]].flatten())
                # print(&#34;moving&#34;)
                # print(b[:].flatten())
                # print(&#34;tensordot&#34;)
                # print((a_with_pad[:, x:x + b.shape[1], y:y + b.shape[2]]).shape)
                # print(b[:].shape)                
                dot = np.tensordot(a_with_pad[:, x:x + b.shape[1], y:y + b.shape[2]], b, axes=([1,2], [1,2]))
                # dot = np.dot(a_with_pad[:, x:x + b.shape[1], y:y + b.shape[2]].flatten(), b[:].flatten())
                # print(&#34;dot.shape&#34;)                
                # print(dot)
                # print(out.shape)
                # print(dot)
                # print(f&#34;{x, y} full conv saving&#34;)
                # print(out)
                out[:,x,y] = dot
                # print(out)
        # print(&#34;Done full convolution&#34;)
        # print(&#34;a_with_pad&#34;)
        # print(a_with_pad)
        # print(&#34;b&#34;)
        # print(b)
        # print(&#34;out&#34;)
        # print(out)
        print(&#34;out not summed&#34;)
        print(out.round(2))
        if sum_out:
            out = np.sum(out, axis=0)
        return out


    def backward_pass(self, error, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
        print(&#39;output after activation function&#39;)
        print(output.round(2))
        print(&#39;target&#39;)
        print(target.round(2))
        print(&#39;2*(output-target)&#39;)
        print((2*(output-target)).round(2))
        print(f&#39;layer activation {model.layers[self.layer_idx].activation}&#39;)
        print(&#39;Z deriv true&#39;)
        print(model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True))

        if not is_last:
            print(&#34;Not last&#34;)
            print(error.round(2))
            print(error.shape)
            print(&#34;W&#34;)
            w = model.params[&#39;W&#39; + str(self.layer_idx)]
            print(w.round(2))
            print(&#34;180 rotate (two flips):&#34;)
            arr = np.flip(np.flip(w, -2), -1)
            print(arr.round(2))
            print(arr.shape)

            if len(arr.shape) &gt; 3:
                print(&#34;Swap axes:&#34;)
                arr = np.swapaxes(arr, 0,1)
                print(arr.round(2))
                print(arr.shape)
            else: print(f&#34;Not swapping axes. Only {len(arr.shape)}&#34;)

            print(&#34;w&#34;)
            print(w.round(2))
            print(&#34;padding&#34;)
            padding = np.array(error.shape[-2:]) - 1
            print(padding.round(2))
            print(&#34;Full conv&#34;)

            print(w.shape)
            print(&#34;arr&#34;)
            print(arr.round(2))
            print(&#34;error&#34;)
            print(error.shape)
            print(error.round(2))            
            print(&#34;padding&#34;)
            print(padding.round(2))
            error = self.conv(arr, error, padding)

            pass
            # error = np.dot(model.params[&#39;W&#39; + str(self.layer_idx + 1)].T, error) * model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)


        else:
            error = 2 * (output - target) * model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)
            print(&#39;(error) dL/d omega&#39;)
            print(error.round(2))

        # new_params[&#39;W&#39; + str(self.layer_idx)] = np.outer(error, model.params[&#39;A&#39; + str(self.layer_idx - 1)])
        prev_a = model.params[&#39;A&#39; + str(self.layer_idx - 1)]
        print(&#39;prev_a&#39;)
        print(prev_a.round(2))
        out_shape = model.params[&#39;W&#39; + str(self.layer_idx)].shape
        print(&#39;convolve back&#39;)
        print(out_shape)
        print(error.shape)
        print(prev_a.shape)
        print(self.padding)
        print(&#34;done printing&#34;)
        # print(&#34;Using full conv&#34;)
        # print(self.conv(prev_a, error, self.padding))        
        new_w = self.conv(prev_a, error, self.padding)
        print(&#34;new_w&#34;)
        print(new_w)
        new_params[&#39;W&#39; + str(self.layer_idx)] = new_w / np.prod(new_w.shape[1:]) * model.learning_rate
        print(new_w / np.prod(new_w.shape[1:]) * model.learning_rate)
        print(&#39;W edits&#39;)
        print(new_params[&#39;W&#39; + str(self.layer_idx)].round(6))

        print(&#34;B (error)&#34;)
        print(error.round(2))
        print(&#34;B&#34;)
        print(np.sum(error, axis=(1,2)).round(2))
        # Bias should work with this line, but it makes things worse somehow
        # new_params[f&#39;B{self.layer_idx}&#39;] = np.sum(error, axis=(1,2)) / np.prod(error.shape[1:]) * model.learning_rate
        new_params[f&#39;B{self.layer_idx}&#39;] = np.zeros(self.num_filters)

        print(f&#34;new_params[B{self.layer_idx}]&#34;)
        print(new_params[&#39;B&#39; + str(self.layer_idx)].round(2))
        return error


    def __str__(self) -&gt; str:
        return f&#34;&lt;Conv2D layer class object named &#39;{self.name}&#39; with input shape {self.input_shape} and filter shape &#39;{self.kernel_size}&#39;&gt;&#34;</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pysnaike.layers_with_prints.Conv2D.backward_pass"><code class="name flex">
<span>def <span class="ident">backward_pass</span></span>(<span>self, error, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backward_pass(self, error, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
    print(&#39;output after activation function&#39;)
    print(output.round(2))
    print(&#39;target&#39;)
    print(target.round(2))
    print(&#39;2*(output-target)&#39;)
    print((2*(output-target)).round(2))
    print(f&#39;layer activation {model.layers[self.layer_idx].activation}&#39;)
    print(&#39;Z deriv true&#39;)
    print(model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True))

    if not is_last:
        print(&#34;Not last&#34;)
        print(error.round(2))
        print(error.shape)
        print(&#34;W&#34;)
        w = model.params[&#39;W&#39; + str(self.layer_idx)]
        print(w.round(2))
        print(&#34;180 rotate (two flips):&#34;)
        arr = np.flip(np.flip(w, -2), -1)
        print(arr.round(2))
        print(arr.shape)

        if len(arr.shape) &gt; 3:
            print(&#34;Swap axes:&#34;)
            arr = np.swapaxes(arr, 0,1)
            print(arr.round(2))
            print(arr.shape)
        else: print(f&#34;Not swapping axes. Only {len(arr.shape)}&#34;)

        print(&#34;w&#34;)
        print(w.round(2))
        print(&#34;padding&#34;)
        padding = np.array(error.shape[-2:]) - 1
        print(padding.round(2))
        print(&#34;Full conv&#34;)

        print(w.shape)
        print(&#34;arr&#34;)
        print(arr.round(2))
        print(&#34;error&#34;)
        print(error.shape)
        print(error.round(2))            
        print(&#34;padding&#34;)
        print(padding.round(2))
        error = self.conv(arr, error, padding)

        pass
        # error = np.dot(model.params[&#39;W&#39; + str(self.layer_idx + 1)].T, error) * model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)


    else:
        error = 2 * (output - target) * model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)
        print(&#39;(error) dL/d omega&#39;)
        print(error.round(2))

    # new_params[&#39;W&#39; + str(self.layer_idx)] = np.outer(error, model.params[&#39;A&#39; + str(self.layer_idx - 1)])
    prev_a = model.params[&#39;A&#39; + str(self.layer_idx - 1)]
    print(&#39;prev_a&#39;)
    print(prev_a.round(2))
    out_shape = model.params[&#39;W&#39; + str(self.layer_idx)].shape
    print(&#39;convolve back&#39;)
    print(out_shape)
    print(error.shape)
    print(prev_a.shape)
    print(self.padding)
    print(&#34;done printing&#34;)
    # print(&#34;Using full conv&#34;)
    # print(self.conv(prev_a, error, self.padding))        
    new_w = self.conv(prev_a, error, self.padding)
    print(&#34;new_w&#34;)
    print(new_w)
    new_params[&#39;W&#39; + str(self.layer_idx)] = new_w / np.prod(new_w.shape[1:]) * model.learning_rate
    print(new_w / np.prod(new_w.shape[1:]) * model.learning_rate)
    print(&#39;W edits&#39;)
    print(new_params[&#39;W&#39; + str(self.layer_idx)].round(6))

    print(&#34;B (error)&#34;)
    print(error.round(2))
    print(&#34;B&#34;)
    print(np.sum(error, axis=(1,2)).round(2))
    # Bias should work with this line, but it makes things worse somehow
    # new_params[f&#39;B{self.layer_idx}&#39;] = np.sum(error, axis=(1,2)) / np.prod(error.shape[1:]) * model.learning_rate
    new_params[f&#39;B{self.layer_idx}&#39;] = np.zeros(self.num_filters)

    print(f&#34;new_params[B{self.layer_idx}]&#34;)
    print(new_params[&#39;B&#39; + str(self.layer_idx)].round(2))
    return error</code></pre>
</details>
</dd>
<dt id="pysnaike.layers_with_prints.Conv2D.calc_num_params"><code class="name flex">
<span>def <span class="ident">calc_num_params</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate number of tweakable weights and biases in layer.</p>
<p>Output as calculated with the formula:
Filter size prod (m x n) * num channels * num filters + bias (which is num filters)</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Number of parameters.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_num_params(self):
    &#34;&#34;&#34;Calculate number of tweakable weights and biases in layer.

    Output as calculated with the formula:
    Filter size prod (m x n) * num channels * num filters + bias (which is num filters)

    Returns:
        int: Number of parameters.
    &#34;&#34;&#34;

    return np.prod(self.kernel_size) * self.input_shape[0] * self.num_filters + self.num_filters</code></pre>
</details>
</dd>
<dt id="pysnaike.layers_with_prints.Conv2D.calc_output_shape"><code class="name flex">
<span>def <span class="ident">calc_output_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Output shape of convolutional layer.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>[out_x, out_y]</code></dt>
<dd>Numpy array with layer output size x and output size y.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_output_shape(self):
    &#34;&#34;&#34;Output shape of convolutional layer.

    Returns:
        [out_x, out_y]: Numpy array with layer output size x and output size y.
    &#34;&#34;&#34;
    return np.append(self.num_filters, [(self.input_shape[1:] + self.padding * 2 - self.kernel_size) // self.strides + 1])</code></pre>
</details>
</dd>
<dt id="pysnaike.layers_with_prints.Conv2D.calc_padding"><code class="name flex">
<span>def <span class="ident">calc_padding</span></span>(<span>self, padding)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate padding size left and right in x-direction and up and down in y-direction.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>padding</code></strong></dt>
<dd>Either "same" or "valid". "same": Padding is added equally on all sides and output grid has same size as input. "valid": No padding is added.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>Padding in x and y-direction.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_padding(self, padding):
    &#34;&#34;&#34;Calculate padding size left and right in x-direction and up and down in y-direction.

    Args:
        padding: Either &#34;same&#34; or &#34;valid&#34;. &#34;same&#34;: Padding is added equally on all sides and output grid has same size as input. &#34;valid&#34;: No padding is added.

    Returns:
        list: Padding in x and y-direction.
    &#34;&#34;&#34;

    pad = np.array([0,0])
    if padding == &#39;same&#39;: pad = (self.kernel_size - 1) // 2
    return pad</code></pre>
</details>
</dd>
<dt id="pysnaike.layers_with_prints.Conv2D.conv"><code class="name flex">
<span>def <span class="ident">conv</span></span>(<span>self, a, b, padding, sum_out=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Full convolution between matrices <code>a</code> and <code>b</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>a</code></strong></dt>
<dd>Stationary matrix.</dd>
<dt><strong><code>b</code></strong></dt>
<dd>Moving matrix.</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Two numbers representing padding</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>array</code></dt>
<dd>Matrix containing convoluted output.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def conv(self, a, b, padding, sum_out=False):
    &#34;&#34;&#34;Full convolution between matrices `a` and `b`.

    Args:
        a: Stationary matrix.
        b: Moving matrix.
        padding: Two numbers representing padding

    Returns:
        array: Matrix containing convoluted output.
    &#34;&#34;&#34;
    print(&#34;a padd&#34;)                
    a_with_pad = np.zeros((a.shape[0], *(a.shape[2:] + padding * 2)))
    print(a_with_pad.shape)
    print(a_with_pad)

    a_with_pad[:, padding[0] : padding[0] + a.shape[1], padding[1]:padding[1] + a.shape[2]] = a
    print(&#39;full conv a_with_pad&#39;)
    print(a_with_pad)

    # print((b.shape[0], *(a.shape[1:] + padding * 2 - b.shape[1:] + 1)))
    out = np.zeros((b.shape[0], *(a.shape[1:] + padding * 2 - b.shape[1:] + 1)))
    print(&#34;out&#34;)
    print(out.shape)
    print(out)

    print(&#34;a matrix&#34;)
    print(a.round(2))
    print(&#34;b matrix&#34;)
    print(b.round(2))
    for x in range(out.shape[1]):
        for y in range(out.shape[2]):
            # print(f&#34;x,y: {x,y}&#34;)                
            # print(a_with_pad.shape)
            # print(a.shape)
            # print(&#34;not moving&#34;)
            # print(a_with_pad[:, x:x + b.shape[1], y:y + b.shape[2]].flatten())
            # print(&#34;moving&#34;)
            # print(b[:].flatten())
            # print(&#34;tensordot&#34;)
            # print((a_with_pad[:, x:x + b.shape[1], y:y + b.shape[2]]).shape)
            # print(b[:].shape)                
            dot = np.tensordot(a_with_pad[:, x:x + b.shape[1], y:y + b.shape[2]], b, axes=([1,2], [1,2]))
            # dot = np.dot(a_with_pad[:, x:x + b.shape[1], y:y + b.shape[2]].flatten(), b[:].flatten())
            # print(&#34;dot.shape&#34;)                
            # print(dot)
            # print(out.shape)
            # print(dot)
            # print(f&#34;{x, y} full conv saving&#34;)
            # print(out)
            out[:,x,y] = dot
            # print(out)
    # print(&#34;Done full convolution&#34;)
    # print(&#34;a_with_pad&#34;)
    # print(a_with_pad)
    # print(&#34;b&#34;)
    # print(b)
    # print(&#34;out&#34;)
    # print(out)
    print(&#34;out not summed&#34;)
    print(out.round(2))
    if sum_out:
        out = np.sum(out, axis=0)
    return out</code></pre>
</details>
</dd>
<dt id="pysnaike.layers_with_prints.Conv2D.forward_pass"><code class="name flex">
<span>def <span class="ident">forward_pass</span></span>(<span>self, params)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform forward pass through layer based on weights and biases from <code>params</code>.
Model <code>params</code> is updated in place.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Model parameters for all layers.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_pass(self, params):
    &#34;&#34;&#34;Perform forward pass through layer based on weights and biases from `params`. 
    Model `params` is updated in place.

    Args:
        params (dict): Model parameters for all layers.
    &#34;&#34;&#34;
    w = params[f&#39;W{self.layer_idx}&#39;]
    b = params[f&#39;B{self.layer_idx}&#39;]
    prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]
    
    conv = self.conv(prev_a, w, self.padding)
    params[f&#39;Z{self.layer_idx}&#39;] = (conv / np.prod(self.kernel_size)) + b[:, np.newaxis, np.newaxis]
    params[f&#39;A{self.layer_idx}&#39;] = self.activation(params[f&#39;Z{self.layer_idx}&#39;])</code></pre>
</details>
</dd>
<dt id="pysnaike.layers_with_prints.Conv2D.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Setup layer and instantiate weights and biases.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>Weights and biases.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, **kvargs):
    &#34;&#34;&#34;Setup layer and instantiate weights and biases.

    Returns:
        list: Weights and biases.
    &#34;&#34;&#34;

    self.layer_idx = kvargs[&#39;layer_idx&#39;]
    
    w = self.kernel
    if self.kernel is None:
        w = np.random.randn(self.num_filters, self.kernel_size[0], self.kernel_size[1])

    b = np.zeros(self.num_filters)        
    return w, b</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pysnaike.layers_with_prints.Dense"><code class="flex name class">
<span>class <span class="ident">Dense</span></span>
<span>(</span><span>size: int, name: str = 'dense', activation=&lt;function leaky_relu&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Dense layer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of perceptrons in the layer.</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of the input layer.</dd>
<dt><strong><code>activation</code></strong> :&ensp;<code>optional</code></dt>
<dd>Activation function used at this layer.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dense():
    def __init__(self, size: int, name: str = &#34;dense&#34;, activation=activations.leaky_relu):
        &#34;&#34;&#34;Dense layer.

        Args:
            size (int): Number of perceptrons in the layer.
            name (str, optional): Name of the input layer.
            activation (optional): Activation function used at this layer.
        &#34;&#34;&#34;

        self.size = size
        self.name = name
        self.activation = activation

        self.num_params = self.size * 2
        self.output_shape = self.size
        self.layer_idx = None


    def setup(self, **kvargs):
        &#34;&#34;&#34;Called on model compile.

        Returns:
            (w, b): Weight and bias.
        &#34;&#34;&#34;

        self.layer_idx = kvargs[&#39;layer_idx&#39;]
        w = np.random.randn(kvargs[&#39;size_curr&#39;], kvargs[&#39;size_prev&#39;]) * 0.1
        b = np.zeros(kvargs[&#39;size_curr&#39;])
        return w, b

    def forward_pass(self, params):
        &#34;&#34;&#34;Forward pass through dense layer.

        Args:
            params (dict): Model parameters.
        &#34;&#34;&#34;
        w = params[f&#39;W{self.layer_idx}&#39;]
        b = params[f&#39;B{self.layer_idx}&#39;]
        prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]

        # Referenced object `params` is modified
        params[f&#39;Z{self.layer_idx}&#39;] = np.dot(w, prev_a) + b
        params[f&#39;A{self.layer_idx}&#39;] = self.activation(params[f&#39;Z{self.layer_idx}&#39;])

    def backward_pass(self, error, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):

        if not is_last:
            error = np.dot(model.params[&#39;W&#39; + str(self.layer_idx + 1)].T, error) * model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)
        else:
            error = 2 * (output - target) / output.shape[0] * model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)

        new_params[&#39;W&#39; + str(self.layer_idx)] = np.outer(error, model.params[&#39;A&#39; + str(self.layer_idx - 1)])
        new_params[&#39;B&#39; + str(self.layer_idx)] = error
        return error


    def __str__(self) -&gt; str:
        return f&#34;&lt;Dense layer class object named &#39;{self.name}&#39; of size {self.size} using activation function &#39;{self.activation.__name__}&#39;&gt;&#34;</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pysnaike.layers_with_prints.Dense.backward_pass"><code class="name flex">
<span>def <span class="ident">backward_pass</span></span>(<span>self, error, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backward_pass(self, error, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):

    if not is_last:
        error = np.dot(model.params[&#39;W&#39; + str(self.layer_idx + 1)].T, error) * model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)
    else:
        error = 2 * (output - target) / output.shape[0] * model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)

    new_params[&#39;W&#39; + str(self.layer_idx)] = np.outer(error, model.params[&#39;A&#39; + str(self.layer_idx - 1)])
    new_params[&#39;B&#39; + str(self.layer_idx)] = error
    return error</code></pre>
</details>
</dd>
<dt id="pysnaike.layers_with_prints.Dense.forward_pass"><code class="name flex">
<span>def <span class="ident">forward_pass</span></span>(<span>self, params)</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass through dense layer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Model parameters.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_pass(self, params):
    &#34;&#34;&#34;Forward pass through dense layer.

    Args:
        params (dict): Model parameters.
    &#34;&#34;&#34;
    w = params[f&#39;W{self.layer_idx}&#39;]
    b = params[f&#39;B{self.layer_idx}&#39;]
    prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]

    # Referenced object `params` is modified
    params[f&#39;Z{self.layer_idx}&#39;] = np.dot(w, prev_a) + b
    params[f&#39;A{self.layer_idx}&#39;] = self.activation(params[f&#39;Z{self.layer_idx}&#39;])</code></pre>
</details>
</dd>
<dt id="pysnaike.layers_with_prints.Dense.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Called on model compile.</p>
<h2 id="returns">Returns</h2>
<p>(w, b): Weight and bias.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, **kvargs):
    &#34;&#34;&#34;Called on model compile.

    Returns:
        (w, b): Weight and bias.
    &#34;&#34;&#34;

    self.layer_idx = kvargs[&#39;layer_idx&#39;]
    w = np.random.randn(kvargs[&#39;size_curr&#39;], kvargs[&#39;size_prev&#39;]) * 0.1
    b = np.zeros(kvargs[&#39;size_curr&#39;])
    return w, b</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pysnaike.layers_with_prints.Flatten"><code class="flex name class">
<span>class <span class="ident">Flatten</span></span>
<span>(</span><span>name='flatten')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Flatten():
    def __init__(self, name=&#39;flatten&#39;):
        self.name = name
        self.output_shape = None
        self.num_params = None

    def setup(self, **kvargs):
        self.layer_idx = kvargs[&#39;layer_idx&#39;]

        if kvargs[&#39;size_prev&#39;].shape is not ():
            self.output_shape = np.prod(kvargs[&#39;size_prev&#39;][0:-1])
        else: self.output_shape = kvargs[&#39;size_prev&#39;]
        return None, None

    def forward_pass(self, params):
        pass</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pysnaike.layers_with_prints.Flatten.forward_pass"><code class="name flex">
<span>def <span class="ident">forward_pass</span></span>(<span>self, params)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_pass(self, params):
    pass</code></pre>
</details>
</dd>
<dt id="pysnaike.layers_with_prints.Flatten.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, **kvargs):
    self.layer_idx = kvargs[&#39;layer_idx&#39;]

    if kvargs[&#39;size_prev&#39;].shape is not ():
        self.output_shape = np.prod(kvargs[&#39;size_prev&#39;][0:-1])
    else: self.output_shape = kvargs[&#39;size_prev&#39;]
    return None, None</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pysnaike.layers_with_prints.MaxPooling2D"><code class="flex name class">
<span>class <span class="ident">MaxPooling2D</span></span>
<span>(</span><span>pool_size)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaxPooling2D():
    def __init__(self, pool_size):
        pass</code></pre>
</details>
</dd>
<dt id="pysnaike.layers_with_prints.Reshape"><code class="flex name class">
<span>class <span class="ident">Reshape</span></span>
<span>(</span><span>output_shape, name='reshape')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Reshape():
    def __init__(self, output_shape, name=&#39;reshape&#39;):
        self.name = name
        self.output_shape = np.array(output_shape)
        self.num_params = None
        self.layer_idx = None
        self.activation = None

    def setup(self, **kvargs):
        self.layer_idx = kvargs[&#39;layer_idx&#39;]
        assert np.prod(kvargs[&#39;size_prev&#39;]) == np.prod(self.output_shape), f&#39;Reshape not possible from {kvargs[&#34;size_prev&#34;]} to {self.output_shape}&#39;

        return None, None

    def forward_pass(self, params):
        params[f&#39;Z{self.layer_idx}&#39;] = None
        params[f&#39;A{self.layer_idx}&#39;] = None</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pysnaike.layers_with_prints.Reshape.forward_pass"><code class="name flex">
<span>def <span class="ident">forward_pass</span></span>(<span>self, params)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_pass(self, params):
    params[f&#39;Z{self.layer_idx}&#39;] = None
    params[f&#39;A{self.layer_idx}&#39;] = None</code></pre>
</details>
</dd>
<dt id="pysnaike.layers_with_prints.Reshape.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, **kvargs):
    self.layer_idx = kvargs[&#39;layer_idx&#39;]
    assert np.prod(kvargs[&#39;size_prev&#39;]) == np.prod(self.output_shape), f&#39;Reshape not possible from {kvargs[&#34;size_prev&#34;]} to {self.output_shape}&#39;

    return None, None</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pysnaike" href="index.html">pysnaike</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pysnaike.layers_with_prints.AvgPooling2D" href="#pysnaike.layers_with_prints.AvgPooling2D">AvgPooling2D</a></code></h4>
</li>
<li>
<h4><code><a title="pysnaike.layers_with_prints.Conv2D" href="#pysnaike.layers_with_prints.Conv2D">Conv2D</a></code></h4>
<ul class="two-column">
<li><code><a title="pysnaike.layers_with_prints.Conv2D.backward_pass" href="#pysnaike.layers_with_prints.Conv2D.backward_pass">backward_pass</a></code></li>
<li><code><a title="pysnaike.layers_with_prints.Conv2D.calc_num_params" href="#pysnaike.layers_with_prints.Conv2D.calc_num_params">calc_num_params</a></code></li>
<li><code><a title="pysnaike.layers_with_prints.Conv2D.calc_output_shape" href="#pysnaike.layers_with_prints.Conv2D.calc_output_shape">calc_output_shape</a></code></li>
<li><code><a title="pysnaike.layers_with_prints.Conv2D.calc_padding" href="#pysnaike.layers_with_prints.Conv2D.calc_padding">calc_padding</a></code></li>
<li><code><a title="pysnaike.layers_with_prints.Conv2D.conv" href="#pysnaike.layers_with_prints.Conv2D.conv">conv</a></code></li>
<li><code><a title="pysnaike.layers_with_prints.Conv2D.forward_pass" href="#pysnaike.layers_with_prints.Conv2D.forward_pass">forward_pass</a></code></li>
<li><code><a title="pysnaike.layers_with_prints.Conv2D.setup" href="#pysnaike.layers_with_prints.Conv2D.setup">setup</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pysnaike.layers_with_prints.Dense" href="#pysnaike.layers_with_prints.Dense">Dense</a></code></h4>
<ul class="">
<li><code><a title="pysnaike.layers_with_prints.Dense.backward_pass" href="#pysnaike.layers_with_prints.Dense.backward_pass">backward_pass</a></code></li>
<li><code><a title="pysnaike.layers_with_prints.Dense.forward_pass" href="#pysnaike.layers_with_prints.Dense.forward_pass">forward_pass</a></code></li>
<li><code><a title="pysnaike.layers_with_prints.Dense.setup" href="#pysnaike.layers_with_prints.Dense.setup">setup</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pysnaike.layers_with_prints.Flatten" href="#pysnaike.layers_with_prints.Flatten">Flatten</a></code></h4>
<ul class="">
<li><code><a title="pysnaike.layers_with_prints.Flatten.forward_pass" href="#pysnaike.layers_with_prints.Flatten.forward_pass">forward_pass</a></code></li>
<li><code><a title="pysnaike.layers_with_prints.Flatten.setup" href="#pysnaike.layers_with_prints.Flatten.setup">setup</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pysnaike.layers_with_prints.MaxPooling2D" href="#pysnaike.layers_with_prints.MaxPooling2D">MaxPooling2D</a></code></h4>
</li>
<li>
<h4><code><a title="pysnaike.layers_with_prints.Reshape" href="#pysnaike.layers_with_prints.Reshape">Reshape</a></code></h4>
<ul class="">
<li><code><a title="pysnaike.layers_with_prints.Reshape.forward_pass" href="#pysnaike.layers_with_prints.Reshape.forward_pass">forward_pass</a></code></li>
<li><code><a title="pysnaike.layers_with_prints.Reshape.setup" href="#pysnaike.layers_with_prints.Reshape.setup">setup</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>
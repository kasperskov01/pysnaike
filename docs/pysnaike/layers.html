<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>pysnaike.layers API documentation</title>
<meta name="description" content="Different types of layers in a network." />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>pysnaike.layers</code></h1>
</header>
<section id="section-intro">
<p>Different types of layers in a network.</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#34;&#34;&#34;Different types of layers in a network.&#34;&#34;&#34;


import numpy as np
# np.random.seed(2)
from pysnaike import activations


class Dense():
    def __init__(self, size: int, name: str = &#34;dense&#34;, activation=activations.leaky_relu):
        &#34;&#34;&#34;Dense layer.

        Args:
            size (int): Number of perceptrons in the layer.
            name (str, optional): Name of the input layer.
            activation (optional): Activation function used at this layer.
        &#34;&#34;&#34;
        self.size = size
        self.input_shape = np.array(self.size)
        self.name = name
        self.activation = activation

        self.num_params = self.size * 2
        self.output_shape = self.input_shape
        self.layer_idx = None


    def setup(self, **kvargs):
        &#34;&#34;&#34;Called on model compile.

        Returns:
            (w, b): Weight and bias.
        &#34;&#34;&#34;

        self.layer_idx = kvargs[&#39;layer_idx&#39;]               
        w = np.random.randn(kvargs[&#39;size_curr&#39;], np.prod(kvargs[&#39;size_prev&#39;])) * 0.1
        b = np.zeros(kvargs[&#39;size_curr&#39;])
        return w, b

    def forward_pass(self, params):
        &#34;&#34;&#34;Forward pass through dense layer.

        Args:
            params (dict): Model parameters.
        &#34;&#34;&#34;
        w = params[f&#39;W{self.layer_idx}&#39;]
        b = params[f&#39;B{self.layer_idx}&#39;]        
        prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]        
        
        # Referenced object `params` is modified
        np.dot(w, prev_a)
        params[f&#39;Z{self.layer_idx}&#39;] = np.dot(w, prev_a) + b        
        params[f&#39;A{self.layer_idx}&#39;] = self.activation(params[f&#39;Z{self.layer_idx}&#39;])

    def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
        &#34;&#34;&#34;Performs backpropagation by calculating weight and bias gradients and returning `gradient` which is dL/dx.

        Args:
            gradient: Partial derivative with respect to activations from previous layer.
            is_last (bool, optional): Whether this layer is the last one in the model. Defaults to False.
            output (optional): Output from this layer during forward pass. Defaults to None.
            target (optional): The target output from the model. Only used when `is_last` is True. Defaults to None.
            new_params (dict, optional): Dictionary with model gradients for weights and biases. Defaults to None.
            model (optional): Reference to the `Sequential` model. Defaults to None.

        Returns:
            np.array: dL/dx. How the loss-function is affected by input changes.
        &#34;&#34;&#34;

        activation_deriv = model.layers[self.layer_idx].activation(model.params[f&#39;Z{self.layer_idx}&#39;], derivative=True)        
        if not is_last:            
            local = gradient * activation_deriv
        else:           
            local = 2 * (output - target) / output.shape[0] * activation_deriv
        
        new_params[f&#39;W{self.layer_idx}&#39;] = np.outer(local, model.params[f&#39;A{self.layer_idx - 1}&#39;])                                
        new_params[f&#39;B{self.layer_idx}&#39;] = local                

        out = np.dot(model.params[f&#39;W{self.layer_idx}&#39;].T, local.T)        
        return out

    def __str__(self) -&gt; str:
        return f&#34;&lt;Dense layer class object named &#39;{self.name}&#39; of size {self.size} using activation function &#39;{self.activation.__name__}&#39;&gt;&#34;


class Conv2D():
    def __init__(self, num_filters, kernel_size=(3, 3), input_shape=(1, 5, 5), strides=(1,1), kernel=None, padding=&#34;same&#34;, activation=activations.identity, name: str = &#34;conv2d&#34;):
        &#34;&#34;&#34;2D convolutional layer.

        Not sure activations are working properly.

        Args:
            num_filters (int): Number of filters in layer.
            kernel_size ((x, y), optional): Size of kernel Both dimensions must be odd. Defaults to (3, 3).
            input_shape ((channels, x, y), optional): Shape of input data. Defaults to (1, 5, 5).
            strides ((x, y), optional): NOT implemented. Step size when sliding across the grid. Defaults to (1, 1).
            kernel (optional): Weights for kernel. Must have shape `(num_filters, *input_shape)`. If nothing is provided, weights will be instantiated randomly. Defaults to None.
            padding (optional): Either &#34;same&#34; or &#34;valid&#34;. &#34;same&#34; means padding is added evenly on all sides, so that output shape equals input shape. &#34;valid&#34; is the same as no padding. Defaults to &#34;same&#34;.
            activation (optional): Activation function used at this layer.
            name (str, optional): Name of the layer. Defaults to &#34;conv2d&#34;.

        Todo:
            Stride.
            Check input shapes.
        &#34;&#34;&#34;

        self.num_filters = num_filters
        self.kernel_size = np.array(kernel_size)
        self.input_shape = np.array(input_shape)
        self.strides = np.array(strides)
        self.kernel = kernel
        self.padding = self.calc_padding(padding)
        self.activation = activation
        self.name = name

        self.layer_idx = 0  # Updated at `Sequential.compile()`
        self.num_params = self.calc_num_params()
        self.output_shape = self.calc_output_shape()

    def setup(self, **kvargs):
        &#34;&#34;&#34;Setup layer and instantiate weights and biases.

        Returns:
            list: Weights and biases.
        &#34;&#34;&#34;

        self.layer_idx = kvargs[&#39;layer_idx&#39;]

        w = self.kernel
        out_shape = (self.num_filters, self.input_shape[0], self.kernel_size[0], self.kernel_size[1])

        if w is not None:
            assert w.shape == out_shape
        else:
            w = np.random.randn(*out_shape)

        b = np.zeros(self.num_filters)
        return w, b

    def calc_padding(self, padding):
        &#34;&#34;&#34;Calculate padding size left and right in x-direction and up and down in y-direction.

        Args:
            padding: Either &#34;same&#34; or &#34;valid&#34;. &#34;same&#34;: Padding is added equally on all sides and output grid has same size as input. &#34;valid&#34;: No padding is added.

        Returns:
            list: Padding in x and y-direction.
        &#34;&#34;&#34;

        pad = np.array([0,0])
        if padding == &#39;same&#39;: pad = (self.kernel_size - 1) // 2
        return pad

    def calc_num_params(self):
        &#34;&#34;&#34;Calculate number of tweakable weights and biases in layer.

        Output as calculated with the formula:
        Filter size prod (m x n) * num channels * num filters + bias (which is num filters)

        Returns:
            int: Number of parameters.
        &#34;&#34;&#34;

        return np.prod(self.kernel_size) * self.input_shape[0] * self.num_filters + self.num_filters

    def calc_output_shape(self):
        &#34;&#34;&#34;Output shape of convolutional layer.

        Returns:
            [out_x, out_y]: Numpy array with layer output size x and output size y.
        &#34;&#34;&#34;
        return np.append(self.num_filters, [(self.input_shape[1:] + self.padding * 2 - self.kernel_size) // self.strides + 1])


    def forward_pass(self, params):
        &#34;&#34;&#34;Perform forward pass through layer based on weights and biases from `params`.
        Model `params` is updated in place.

        Args:
            params (dict): Model parameters for all layers.
        &#34;&#34;&#34;

        w = params[f&#39;W{self.layer_idx}&#39;]
        b = params[f&#39;B{self.layer_idx}&#39;]
        prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]
        conv = self.convolve(prev_a, w, self.padding)
        params[f&#39;Z{self.layer_idx}&#39;] = (conv / np.prod(self.kernel_size)) + b[:, np.newaxis, np.newaxis]        
        params[f&#39;A{self.layer_idx}&#39;] = self.activation(params[f&#39;Z{self.layer_idx}&#39;])



    def convolve(self, a, b, padding, sum_out=False, back_conv = False, full_conv=False):
        &#34;&#34;&#34;Full convolution between matrices `a` and `b`.

        Args:
            a: Stationary matrix.
            b: Moving matrix.
            padding: Two numbers representing padding.
            sum_out (bool, optional): Whether output matrix should be summed along axis 0. Defaults to False.

        Returns:
            array: Matrix containing convoluted output.
        &#34;&#34;&#34;       
        a_with_pad = np.zeros((a.shape[0], *(a.shape[1:] + padding * 2)))        
        a_with_pad[:, padding[0] : padding[0] + a.shape[1], padding[1]:padding[1] + a.shape[2]] = a

        if not back_conv:
            out = np.zeros((b.shape[0], *(a.shape[2:] + padding * 2 - b.shape[2:] + 1)))
        else:
            out = np.zeros((b.shape[0], a.shape[0], *(a.shape[2:] + padding * 2 - b.shape[2:] + 1)))

        for x in range(out.shape[-2]):
            for y in range(out.shape[-1]):
                if not back_conv:
                    dot = np.tensordot(a_with_pad[:, x: x + b.shape[2], y: y + b.shape[3]], b, axes=([0,1,2], [1,2,3]))
                    out[:,x,y] = dot
                else:
                    dot = np.tensordot(a_with_pad[:, x: x + b.shape[1], y: y + b.shape[2]], b, axes=([1,2], [1,2]))
                    out[:,:,x,y] = dot.T
        if sum_out:
            out = np.sum(out, axis=0)
        return out

    def full_back_conv(self, a, b, padding, next_padding, sum_out=False, back_conv = False, full_conv=False):
        &#34;&#34;&#34;Full convolution between matrices `a` and `b`.

        Args:
            a: Stationary matrix.
            b: Moving matrix.
            padding: Two numbers representing padding.
            sum_out (bool, optional): Whether output matrix should be summed along axis 0. Defaults to False.

        Returns:
            array: Matrix containing convoluted output.
        &#34;&#34;&#34;
        a_with_pad = np.zeros((*a.shape[:-2], *(a.shape[-2:] + padding * 2)))
        a_with_pad[:,:, padding[0] : padding[0] + a.shape[-2], padding[1]:padding[1] + a.shape[-1]] = a

        if not back_conv:
            out = np.zeros((a.shape[0], *(a.shape[2:] + padding * 2 - b.shape[2:] + 1)))
        else:
            out = np.zeros((b.shape[0], a.shape[0], *(a.shape[2:] + padding * 2 - b.shape[2:] + 1)))

        for x in range(out.shape[-2]):
            for y in range(out.shape[-1]):
                dot = np.tensordot(a_with_pad[:, :, x: x + b.shape[1], y: y + b.shape[2]], b, axes=([1,2,3], [0,1,2]))
                out[:,x,y] = dot

        if sum_out:
            out = np.sum(out, axis=0)

        # Remove original padding
        out = out[:,next_padding[0]:-next_padding[0], next_padding[1]:-next_padding[1]]
        return out


    def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
        &#34;&#34;&#34;Performs backpropagation by calculating weight and bias gradients and returning `gradient` which is dL/dx.
            Possible gradient. Not keeping track of layers.


        Args:
            gradient: Partial derivative of the final loss function with respect to activations from n+1 layer.
            is_last (bool, optional): Whether this layer is the last one in the model. Defaults to False.
            output (optional): Output from this layer during forward pass. Defaults to None.
            target (optional): The target output from the model. Only used when `is_last` is True. Defaults to None.
            new_params (dict, optional): Dictionary with model gradients for weights and biases. Defaults to None.
            model (optional): Reference to the `Sequential` model. Defaults to None.

        Returns:
            np.array: dL/dx. How the loss-function is affected by input changes.
        &#34;&#34;&#34;

        activation_deriv = model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)
        if not is_last:
            local = gradient * activation_deriv
        else:
            local = 2 * (output - target) * activation_deriv

        prev_a = model.params[&#39;A&#39; + str(self.layer_idx - 1)]
        new_w = self.convolve(prev_a, local, self.padding, back_conv=True)
        new_params[&#39;W&#39; + str(self.layer_idx)] = new_w / np.prod(new_w.shape[1:]) * model.learning_rate
        new_params[f&#39;B{self.layer_idx}&#39;] = np.zeros(self.num_filters)

        # prepare gradient to return from this layer
        w = model.params[&#39;W&#39; + str(self.layer_idx)]
        arr = np.flip(np.flip(w, -2), -1)
        if len(arr.shape) &gt; 3:
            arr = np.swapaxes(arr, 0,1)
        else: print(&#34;didn&#39;t swap axes&#34;)

        out_padding = np.array(local.shape[-2:]) - 1
        in_padding = self.padding

        return self.full_back_conv(arr, local, out_padding, in_padding, full_conv=True)

    def __str__(self) -&gt; str:
        return f&#34;&lt;Conv2D layer class object named &#39;{self.name}&#39; with input shape {self.input_shape} and filter shape &#39;{self.kernel_size}&#39;&gt;&#34;


class MaxPooling2D():
    def __init__(self, pool_size, name=&#34;max_pool&#34;):
        self.pool_size = np.array(pool_size)
        self.name = name
        self.output_shape = None
        self.num_params = None

    def setup(self, **kvargs):
        self.layer_idx = kvargs[&#39;layer_idx&#39;]
        self.input_shape = kvargs[&#39;size_prev&#39;]
        self.output_shape = (self.input_shape[0], *(self.input_shape[-2:] // self.pool_size))

        # Updating pool size with prev layer channel dimension
        self.pool_size = np.array((self.input_shape[0], *self.pool_size))
        idx = np.ones(self.input_shape)
        return idx, None

    def pool(self, a, b, out_shape):
        &#34;&#34;&#34;Convolve `b` across `a`.

        Args:
            a: Stationary matrix.
            b: Moving pool matrix shape.

        Todo:
            Use numpy slide and stride. [::]
        &#34;&#34;&#34;
        
        stride = b[-2:]

        out = np.zeros(out_shape)
        out_coords = np.zeros((*out.shape, 2))

        for x in range(out.shape[-2]):
            for y in range(out.shape[-1]):
                arr_x = x * stride[0]
                arr_y = y * stride[1]

                curr_a = a[:, arr_x : arr_x + stride[0], arr_y : arr_y + stride[1]]
                curr_a_reshape = curr_a.reshape(curr_a.shape[0], -1)
                max = curr_a_reshape.argmax(axis=1)
                coords = np.column_stack(np.unravel_index(max, curr_a.shape[1:]))
                vals = curr_a_reshape[np.arange(curr_a_reshape.shape[0]), max]

                coords = coords.T
                coords[0] += arr_x
                coords[1] += arr_y
                out_coords[:,x,y,:] = coords.T
                out[:,x,y] = vals

        return out_coords, out


    def forward_pass(self, params):

        prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]
        out_coords, out = self.pool(prev_a, self.pool_size, self.output_shape)        
        for channel in range(out.shape[0]):
            max = np.max(out[channel])           
            min = np.min(out[channel])      
            divisor = np.max([max, np.absolute(min)])
            out[channel] /= divisor            
        
        params[f&#39;A{self.layer_idx}&#39;] = out
        params[f&#39;I{self.layer_idx}&#39;] = out_coords.astype(int)
        
    def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
        &#34;&#34;&#34;Performs backpropagation by calculating weight and bias gradients and returning `gradient` which is dL/dx.

        Args:
            gradient: Partial derivative with respect to activations from previous layer.
            is_last (bool, optional): Whether this layer is the last one in the model. Defaults to False.
            output (optional): Output from this layer during forward pass. Defaults to None.
            target (optional): The target output from the model. Only used when `is_last` is True. Defaults to None.
            new_params (dict, optional): Dictionary with model gradients for weights and biases. Defaults to None.
            model (optional): Reference to the `Sequential` model. Defaults to None.

        Returns:
            np.array: dL/dx. How the loss-function is affected by input changes.
        &#34;&#34;&#34;

        if not is_last:
            local = gradient
        else:
            local = 2 * (output - target)
        
        arr_idx = model.params[&#39;I&#39; + str(self.layer_idx)].copy()
        out_gradient = np.ones(self.input_shape)
        for n in range(arr_idx.shape[0]):
            n_coords = np.concatenate(arr_idx[n], 0)
            n_coords = n_coords.T
            out_gradient[n, n_coords[0], n_coords[1]] = local[n].flatten()

        return out_gradient


class AvgPooling2D():
    def __init__(self, pool_size):
        pass


class Flatten():
    def __init__(self, name=&#39;flatten&#39;):
        self.name = name
        self.output_shape = None
        self.num_params = None
        self.size_prev = None

    def setup(self, **kvargs):
        self.layer_idx = kvargs[&#39;layer_idx&#39;]
        self.size_prev = kvargs[&#39;size_prev&#39;]
        self.output_shape = np.array([np.prod(self.size_prev)])
        return None, None

    def forward_pass(self, params):        
        params[f&#39;A{self.layer_idx}&#39;] = params[f&#39;A{self.layer_idx - 1}&#39;].flatten()

    def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
        if not is_last:
            local = gradient
        else:
            local = 2 * (output - target)

        return local.reshape(self.size_prev)


class Reshape():
    def __init__(self, output_shape, name=&#39;reshape&#39;):
        self.name = name
        self.input_shape = np.array([0])
        self.output_shape = output_shape
        self.num_params = None
        self.size_prev = None        

    def setup(self, **kvargs):
        self.layer_idx = kvargs[&#39;layer_idx&#39;]
        self.size_prev = kvargs[&#39;size_prev&#39;]
        return None, None

    def forward_pass(self, params):        
        params[f&#39;A{self.layer_idx}&#39;] = params[f&#39;A{self.layer_idx - 1}&#39;].reshape(self.output_shape)
        
    def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
        if not is_last:
            local = gradient
        else:
            local = 2 * (output - target)

        return local.reshape(self.size_prev)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="pysnaike.layers.AvgPooling2D"><code class="flex name class">
<span>class <span class="ident">AvgPooling2D</span></span>
<span>(</span><span>pool_size)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class AvgPooling2D():
    def __init__(self, pool_size):
        pass</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.Conv2D"><code class="flex name class">
<span>class <span class="ident">Conv2D</span></span>
<span>(</span><span>num_filters, kernel_size=(3, 3), input_shape=(1, 5, 5), strides=(1, 1), kernel=None, padding='same', activation=&lt;function identity&gt;, name: str = 'conv2d')</span>
</code></dt>
<dd>
<div class="desc"><p>2D convolutional layer.</p>
<p>Not sure activations are working properly.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>num_filters</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of filters in layer.</dd>
<dt>kernel_size ((x, y), optional): Size of kernel Both dimensions must be odd. Defaults to (3, 3).</dt>
<dt>input_shape ((channels, x, y), optional): Shape of input data. Defaults to (1, 5, 5).</dt>
<dt>strides ((x, y), optional): NOT implemented. Step size when sliding across the grid. Defaults to (1, 1).</dt>
<dt><strong><code>kernel</code></strong> :&ensp;<code>optional</code></dt>
<dd>Weights for kernel. Must have shape <code>(num_filters, *input_shape)</code>. If nothing is provided, weights will be instantiated randomly. Defaults to None.</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>optional</code></dt>
<dd>Either "same" or "valid". "same" means padding is added evenly on all sides, so that output shape equals input shape. "valid" is the same as no padding. Defaults to "same".</dd>
<dt><strong><code>activation</code></strong> :&ensp;<code>optional</code></dt>
<dd>Activation function used at this layer.</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of the layer. Defaults to "conv2d".</dd>
</dl>
<h2 id="todo">Todo</h2>
<p>Stride.
Check input shapes.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Conv2D():
    def __init__(self, num_filters, kernel_size=(3, 3), input_shape=(1, 5, 5), strides=(1,1), kernel=None, padding=&#34;same&#34;, activation=activations.identity, name: str = &#34;conv2d&#34;):
        &#34;&#34;&#34;2D convolutional layer.

        Not sure activations are working properly.

        Args:
            num_filters (int): Number of filters in layer.
            kernel_size ((x, y), optional): Size of kernel Both dimensions must be odd. Defaults to (3, 3).
            input_shape ((channels, x, y), optional): Shape of input data. Defaults to (1, 5, 5).
            strides ((x, y), optional): NOT implemented. Step size when sliding across the grid. Defaults to (1, 1).
            kernel (optional): Weights for kernel. Must have shape `(num_filters, *input_shape)`. If nothing is provided, weights will be instantiated randomly. Defaults to None.
            padding (optional): Either &#34;same&#34; or &#34;valid&#34;. &#34;same&#34; means padding is added evenly on all sides, so that output shape equals input shape. &#34;valid&#34; is the same as no padding. Defaults to &#34;same&#34;.
            activation (optional): Activation function used at this layer.
            name (str, optional): Name of the layer. Defaults to &#34;conv2d&#34;.

        Todo:
            Stride.
            Check input shapes.
        &#34;&#34;&#34;

        self.num_filters = num_filters
        self.kernel_size = np.array(kernel_size)
        self.input_shape = np.array(input_shape)
        self.strides = np.array(strides)
        self.kernel = kernel
        self.padding = self.calc_padding(padding)
        self.activation = activation
        self.name = name

        self.layer_idx = 0  # Updated at `Sequential.compile()`
        self.num_params = self.calc_num_params()
        self.output_shape = self.calc_output_shape()

    def setup(self, **kvargs):
        &#34;&#34;&#34;Setup layer and instantiate weights and biases.

        Returns:
            list: Weights and biases.
        &#34;&#34;&#34;

        self.layer_idx = kvargs[&#39;layer_idx&#39;]

        w = self.kernel
        out_shape = (self.num_filters, self.input_shape[0], self.kernel_size[0], self.kernel_size[1])

        if w is not None:
            assert w.shape == out_shape
        else:
            w = np.random.randn(*out_shape)

        b = np.zeros(self.num_filters)
        return w, b

    def calc_padding(self, padding):
        &#34;&#34;&#34;Calculate padding size left and right in x-direction and up and down in y-direction.

        Args:
            padding: Either &#34;same&#34; or &#34;valid&#34;. &#34;same&#34;: Padding is added equally on all sides and output grid has same size as input. &#34;valid&#34;: No padding is added.

        Returns:
            list: Padding in x and y-direction.
        &#34;&#34;&#34;

        pad = np.array([0,0])
        if padding == &#39;same&#39;: pad = (self.kernel_size - 1) // 2
        return pad

    def calc_num_params(self):
        &#34;&#34;&#34;Calculate number of tweakable weights and biases in layer.

        Output as calculated with the formula:
        Filter size prod (m x n) * num channels * num filters + bias (which is num filters)

        Returns:
            int: Number of parameters.
        &#34;&#34;&#34;

        return np.prod(self.kernel_size) * self.input_shape[0] * self.num_filters + self.num_filters

    def calc_output_shape(self):
        &#34;&#34;&#34;Output shape of convolutional layer.

        Returns:
            [out_x, out_y]: Numpy array with layer output size x and output size y.
        &#34;&#34;&#34;
        return np.append(self.num_filters, [(self.input_shape[1:] + self.padding * 2 - self.kernel_size) // self.strides + 1])


    def forward_pass(self, params):
        &#34;&#34;&#34;Perform forward pass through layer based on weights and biases from `params`.
        Model `params` is updated in place.

        Args:
            params (dict): Model parameters for all layers.
        &#34;&#34;&#34;

        w = params[f&#39;W{self.layer_idx}&#39;]
        b = params[f&#39;B{self.layer_idx}&#39;]
        prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]
        conv = self.convolve(prev_a, w, self.padding)
        params[f&#39;Z{self.layer_idx}&#39;] = (conv / np.prod(self.kernel_size)) + b[:, np.newaxis, np.newaxis]        
        params[f&#39;A{self.layer_idx}&#39;] = self.activation(params[f&#39;Z{self.layer_idx}&#39;])



    def convolve(self, a, b, padding, sum_out=False, back_conv = False, full_conv=False):
        &#34;&#34;&#34;Full convolution between matrices `a` and `b`.

        Args:
            a: Stationary matrix.
            b: Moving matrix.
            padding: Two numbers representing padding.
            sum_out (bool, optional): Whether output matrix should be summed along axis 0. Defaults to False.

        Returns:
            array: Matrix containing convoluted output.
        &#34;&#34;&#34;       
        a_with_pad = np.zeros((a.shape[0], *(a.shape[1:] + padding * 2)))        
        a_with_pad[:, padding[0] : padding[0] + a.shape[1], padding[1]:padding[1] + a.shape[2]] = a

        if not back_conv:
            out = np.zeros((b.shape[0], *(a.shape[2:] + padding * 2 - b.shape[2:] + 1)))
        else:
            out = np.zeros((b.shape[0], a.shape[0], *(a.shape[2:] + padding * 2 - b.shape[2:] + 1)))

        for x in range(out.shape[-2]):
            for y in range(out.shape[-1]):
                if not back_conv:
                    dot = np.tensordot(a_with_pad[:, x: x + b.shape[2], y: y + b.shape[3]], b, axes=([0,1,2], [1,2,3]))
                    out[:,x,y] = dot
                else:
                    dot = np.tensordot(a_with_pad[:, x: x + b.shape[1], y: y + b.shape[2]], b, axes=([1,2], [1,2]))
                    out[:,:,x,y] = dot.T
        if sum_out:
            out = np.sum(out, axis=0)
        return out

    def full_back_conv(self, a, b, padding, next_padding, sum_out=False, back_conv = False, full_conv=False):
        &#34;&#34;&#34;Full convolution between matrices `a` and `b`.

        Args:
            a: Stationary matrix.
            b: Moving matrix.
            padding: Two numbers representing padding.
            sum_out (bool, optional): Whether output matrix should be summed along axis 0. Defaults to False.

        Returns:
            array: Matrix containing convoluted output.
        &#34;&#34;&#34;
        a_with_pad = np.zeros((*a.shape[:-2], *(a.shape[-2:] + padding * 2)))
        a_with_pad[:,:, padding[0] : padding[0] + a.shape[-2], padding[1]:padding[1] + a.shape[-1]] = a

        if not back_conv:
            out = np.zeros((a.shape[0], *(a.shape[2:] + padding * 2 - b.shape[2:] + 1)))
        else:
            out = np.zeros((b.shape[0], a.shape[0], *(a.shape[2:] + padding * 2 - b.shape[2:] + 1)))

        for x in range(out.shape[-2]):
            for y in range(out.shape[-1]):
                dot = np.tensordot(a_with_pad[:, :, x: x + b.shape[1], y: y + b.shape[2]], b, axes=([1,2,3], [0,1,2]))
                out[:,x,y] = dot

        if sum_out:
            out = np.sum(out, axis=0)

        # Remove original padding
        out = out[:,next_padding[0]:-next_padding[0], next_padding[1]:-next_padding[1]]
        return out


    def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
        &#34;&#34;&#34;Performs backpropagation by calculating weight and bias gradients and returning `gradient` which is dL/dx.
            Possible gradient. Not keeping track of layers.


        Args:
            gradient: Partial derivative of the final loss function with respect to activations from n+1 layer.
            is_last (bool, optional): Whether this layer is the last one in the model. Defaults to False.
            output (optional): Output from this layer during forward pass. Defaults to None.
            target (optional): The target output from the model. Only used when `is_last` is True. Defaults to None.
            new_params (dict, optional): Dictionary with model gradients for weights and biases. Defaults to None.
            model (optional): Reference to the `Sequential` model. Defaults to None.

        Returns:
            np.array: dL/dx. How the loss-function is affected by input changes.
        &#34;&#34;&#34;

        activation_deriv = model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)
        if not is_last:
            local = gradient * activation_deriv
        else:
            local = 2 * (output - target) * activation_deriv

        prev_a = model.params[&#39;A&#39; + str(self.layer_idx - 1)]
        new_w = self.convolve(prev_a, local, self.padding, back_conv=True)
        new_params[&#39;W&#39; + str(self.layer_idx)] = new_w / np.prod(new_w.shape[1:]) * model.learning_rate
        new_params[f&#39;B{self.layer_idx}&#39;] = np.zeros(self.num_filters)

        # prepare gradient to return from this layer
        w = model.params[&#39;W&#39; + str(self.layer_idx)]
        arr = np.flip(np.flip(w, -2), -1)
        if len(arr.shape) &gt; 3:
            arr = np.swapaxes(arr, 0,1)
        else: print(&#34;didn&#39;t swap axes&#34;)

        out_padding = np.array(local.shape[-2:]) - 1
        in_padding = self.padding

        return self.full_back_conv(arr, local, out_padding, in_padding, full_conv=True)

    def __str__(self) -&gt; str:
        return f&#34;&lt;Conv2D layer class object named &#39;{self.name}&#39; with input shape {self.input_shape} and filter shape &#39;{self.kernel_size}&#39;&gt;&#34;</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pysnaike.layers.Conv2D.backward_pass"><code class="name flex">
<span>def <span class="ident">backward_pass</span></span>(<span>self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs backpropagation by calculating weight and bias gradients and returning <code>gradient</code> which is dL/dx.
Possible gradient. Not keeping track of layers.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gradient</code></strong></dt>
<dd>Partial derivative of the final loss function with respect to activations from n+1 layer.</dd>
<dt><strong><code>is_last</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether this layer is the last one in the model. Defaults to False.</dd>
<dt><strong><code>output</code></strong> :&ensp;<code>optional</code></dt>
<dd>Output from this layer during forward pass. Defaults to None.</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>optional</code></dt>
<dd>The target output from the model. Only used when <code>is_last</code> is True. Defaults to None.</dd>
<dt><strong><code>new_params</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Dictionary with model gradients for weights and biases. Defaults to None.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>optional</code></dt>
<dd>Reference to the <code>Sequential</code> model. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>dL/dx. How the loss-function is affected by input changes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
    &#34;&#34;&#34;Performs backpropagation by calculating weight and bias gradients and returning `gradient` which is dL/dx.
        Possible gradient. Not keeping track of layers.


    Args:
        gradient: Partial derivative of the final loss function with respect to activations from n+1 layer.
        is_last (bool, optional): Whether this layer is the last one in the model. Defaults to False.
        output (optional): Output from this layer during forward pass. Defaults to None.
        target (optional): The target output from the model. Only used when `is_last` is True. Defaults to None.
        new_params (dict, optional): Dictionary with model gradients for weights and biases. Defaults to None.
        model (optional): Reference to the `Sequential` model. Defaults to None.

    Returns:
        np.array: dL/dx. How the loss-function is affected by input changes.
    &#34;&#34;&#34;

    activation_deriv = model.layers[self.layer_idx].activation(model.params[&#39;Z&#39; + str(self.layer_idx)], derivative=True)
    if not is_last:
        local = gradient * activation_deriv
    else:
        local = 2 * (output - target) * activation_deriv

    prev_a = model.params[&#39;A&#39; + str(self.layer_idx - 1)]
    new_w = self.convolve(prev_a, local, self.padding, back_conv=True)
    new_params[&#39;W&#39; + str(self.layer_idx)] = new_w / np.prod(new_w.shape[1:]) * model.learning_rate
    new_params[f&#39;B{self.layer_idx}&#39;] = np.zeros(self.num_filters)

    # prepare gradient to return from this layer
    w = model.params[&#39;W&#39; + str(self.layer_idx)]
    arr = np.flip(np.flip(w, -2), -1)
    if len(arr.shape) &gt; 3:
        arr = np.swapaxes(arr, 0,1)
    else: print(&#34;didn&#39;t swap axes&#34;)

    out_padding = np.array(local.shape[-2:]) - 1
    in_padding = self.padding

    return self.full_back_conv(arr, local, out_padding, in_padding, full_conv=True)</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.Conv2D.calc_num_params"><code class="name flex">
<span>def <span class="ident">calc_num_params</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate number of tweakable weights and biases in layer.</p>
<p>Output as calculated with the formula:
Filter size prod (m x n) * num channels * num filters + bias (which is num filters)</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>int</code></dt>
<dd>Number of parameters.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_num_params(self):
    &#34;&#34;&#34;Calculate number of tweakable weights and biases in layer.

    Output as calculated with the formula:
    Filter size prod (m x n) * num channels * num filters + bias (which is num filters)

    Returns:
        int: Number of parameters.
    &#34;&#34;&#34;

    return np.prod(self.kernel_size) * self.input_shape[0] * self.num_filters + self.num_filters</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.Conv2D.calc_output_shape"><code class="name flex">
<span>def <span class="ident">calc_output_shape</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>Output shape of convolutional layer.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>[out_x, out_y]</code></dt>
<dd>Numpy array with layer output size x and output size y.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_output_shape(self):
    &#34;&#34;&#34;Output shape of convolutional layer.

    Returns:
        [out_x, out_y]: Numpy array with layer output size x and output size y.
    &#34;&#34;&#34;
    return np.append(self.num_filters, [(self.input_shape[1:] + self.padding * 2 - self.kernel_size) // self.strides + 1])</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.Conv2D.calc_padding"><code class="name flex">
<span>def <span class="ident">calc_padding</span></span>(<span>self, padding)</span>
</code></dt>
<dd>
<div class="desc"><p>Calculate padding size left and right in x-direction and up and down in y-direction.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>padding</code></strong></dt>
<dd>Either "same" or "valid". "same": Padding is added equally on all sides and output grid has same size as input. "valid": No padding is added.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>Padding in x and y-direction.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def calc_padding(self, padding):
    &#34;&#34;&#34;Calculate padding size left and right in x-direction and up and down in y-direction.

    Args:
        padding: Either &#34;same&#34; or &#34;valid&#34;. &#34;same&#34;: Padding is added equally on all sides and output grid has same size as input. &#34;valid&#34;: No padding is added.

    Returns:
        list: Padding in x and y-direction.
    &#34;&#34;&#34;

    pad = np.array([0,0])
    if padding == &#39;same&#39;: pad = (self.kernel_size - 1) // 2
    return pad</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.Conv2D.convolve"><code class="name flex">
<span>def <span class="ident">convolve</span></span>(<span>self, a, b, padding, sum_out=False, back_conv=False, full_conv=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Full convolution between matrices <code>a</code> and <code>b</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>a</code></strong></dt>
<dd>Stationary matrix.</dd>
<dt><strong><code>b</code></strong></dt>
<dd>Moving matrix.</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Two numbers representing padding.</dd>
<dt><strong><code>sum_out</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether output matrix should be summed along axis 0. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>array</code></dt>
<dd>Matrix containing convoluted output.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convolve(self, a, b, padding, sum_out=False, back_conv = False, full_conv=False):
    &#34;&#34;&#34;Full convolution between matrices `a` and `b`.

    Args:
        a: Stationary matrix.
        b: Moving matrix.
        padding: Two numbers representing padding.
        sum_out (bool, optional): Whether output matrix should be summed along axis 0. Defaults to False.

    Returns:
        array: Matrix containing convoluted output.
    &#34;&#34;&#34;       
    a_with_pad = np.zeros((a.shape[0], *(a.shape[1:] + padding * 2)))        
    a_with_pad[:, padding[0] : padding[0] + a.shape[1], padding[1]:padding[1] + a.shape[2]] = a

    if not back_conv:
        out = np.zeros((b.shape[0], *(a.shape[2:] + padding * 2 - b.shape[2:] + 1)))
    else:
        out = np.zeros((b.shape[0], a.shape[0], *(a.shape[2:] + padding * 2 - b.shape[2:] + 1)))

    for x in range(out.shape[-2]):
        for y in range(out.shape[-1]):
            if not back_conv:
                dot = np.tensordot(a_with_pad[:, x: x + b.shape[2], y: y + b.shape[3]], b, axes=([0,1,2], [1,2,3]))
                out[:,x,y] = dot
            else:
                dot = np.tensordot(a_with_pad[:, x: x + b.shape[1], y: y + b.shape[2]], b, axes=([1,2], [1,2]))
                out[:,:,x,y] = dot.T
    if sum_out:
        out = np.sum(out, axis=0)
    return out</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.Conv2D.forward_pass"><code class="name flex">
<span>def <span class="ident">forward_pass</span></span>(<span>self, params)</span>
</code></dt>
<dd>
<div class="desc"><p>Perform forward pass through layer based on weights and biases from <code>params</code>.
Model <code>params</code> is updated in place.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Model parameters for all layers.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_pass(self, params):
    &#34;&#34;&#34;Perform forward pass through layer based on weights and biases from `params`.
    Model `params` is updated in place.

    Args:
        params (dict): Model parameters for all layers.
    &#34;&#34;&#34;

    w = params[f&#39;W{self.layer_idx}&#39;]
    b = params[f&#39;B{self.layer_idx}&#39;]
    prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]
    conv = self.convolve(prev_a, w, self.padding)
    params[f&#39;Z{self.layer_idx}&#39;] = (conv / np.prod(self.kernel_size)) + b[:, np.newaxis, np.newaxis]        
    params[f&#39;A{self.layer_idx}&#39;] = self.activation(params[f&#39;Z{self.layer_idx}&#39;])</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.Conv2D.full_back_conv"><code class="name flex">
<span>def <span class="ident">full_back_conv</span></span>(<span>self, a, b, padding, next_padding, sum_out=False, back_conv=False, full_conv=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Full convolution between matrices <code>a</code> and <code>b</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>a</code></strong></dt>
<dd>Stationary matrix.</dd>
<dt><strong><code>b</code></strong></dt>
<dd>Moving matrix.</dd>
<dt><strong><code>padding</code></strong></dt>
<dd>Two numbers representing padding.</dd>
<dt><strong><code>sum_out</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether output matrix should be summed along axis 0. Defaults to False.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>array</code></dt>
<dd>Matrix containing convoluted output.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def full_back_conv(self, a, b, padding, next_padding, sum_out=False, back_conv = False, full_conv=False):
    &#34;&#34;&#34;Full convolution between matrices `a` and `b`.

    Args:
        a: Stationary matrix.
        b: Moving matrix.
        padding: Two numbers representing padding.
        sum_out (bool, optional): Whether output matrix should be summed along axis 0. Defaults to False.

    Returns:
        array: Matrix containing convoluted output.
    &#34;&#34;&#34;
    a_with_pad = np.zeros((*a.shape[:-2], *(a.shape[-2:] + padding * 2)))
    a_with_pad[:,:, padding[0] : padding[0] + a.shape[-2], padding[1]:padding[1] + a.shape[-1]] = a

    if not back_conv:
        out = np.zeros((a.shape[0], *(a.shape[2:] + padding * 2 - b.shape[2:] + 1)))
    else:
        out = np.zeros((b.shape[0], a.shape[0], *(a.shape[2:] + padding * 2 - b.shape[2:] + 1)))

    for x in range(out.shape[-2]):
        for y in range(out.shape[-1]):
            dot = np.tensordot(a_with_pad[:, :, x: x + b.shape[1], y: y + b.shape[2]], b, axes=([1,2,3], [0,1,2]))
            out[:,x,y] = dot

    if sum_out:
        out = np.sum(out, axis=0)

    # Remove original padding
    out = out[:,next_padding[0]:-next_padding[0], next_padding[1]:-next_padding[1]]
    return out</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.Conv2D.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Setup layer and instantiate weights and biases.</p>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>list</code></dt>
<dd>Weights and biases.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, **kvargs):
    &#34;&#34;&#34;Setup layer and instantiate weights and biases.

    Returns:
        list: Weights and biases.
    &#34;&#34;&#34;

    self.layer_idx = kvargs[&#39;layer_idx&#39;]

    w = self.kernel
    out_shape = (self.num_filters, self.input_shape[0], self.kernel_size[0], self.kernel_size[1])

    if w is not None:
        assert w.shape == out_shape
    else:
        w = np.random.randn(*out_shape)

    b = np.zeros(self.num_filters)
    return w, b</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pysnaike.layers.Dense"><code class="flex name class">
<span>class <span class="ident">Dense</span></span>
<span>(</span><span>size: int, name: str = 'dense', activation=&lt;function leaky_relu&gt;)</span>
</code></dt>
<dd>
<div class="desc"><p>Dense layer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>size</code></strong> :&ensp;<code>int</code></dt>
<dd>Number of perceptrons in the layer.</dd>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of the input layer.</dd>
<dt><strong><code>activation</code></strong> :&ensp;<code>optional</code></dt>
<dd>Activation function used at this layer.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dense():
    def __init__(self, size: int, name: str = &#34;dense&#34;, activation=activations.leaky_relu):
        &#34;&#34;&#34;Dense layer.

        Args:
            size (int): Number of perceptrons in the layer.
            name (str, optional): Name of the input layer.
            activation (optional): Activation function used at this layer.
        &#34;&#34;&#34;
        self.size = size
        self.input_shape = np.array(self.size)
        self.name = name
        self.activation = activation

        self.num_params = self.size * 2
        self.output_shape = self.input_shape
        self.layer_idx = None


    def setup(self, **kvargs):
        &#34;&#34;&#34;Called on model compile.

        Returns:
            (w, b): Weight and bias.
        &#34;&#34;&#34;

        self.layer_idx = kvargs[&#39;layer_idx&#39;]               
        w = np.random.randn(kvargs[&#39;size_curr&#39;], np.prod(kvargs[&#39;size_prev&#39;])) * 0.1
        b = np.zeros(kvargs[&#39;size_curr&#39;])
        return w, b

    def forward_pass(self, params):
        &#34;&#34;&#34;Forward pass through dense layer.

        Args:
            params (dict): Model parameters.
        &#34;&#34;&#34;
        w = params[f&#39;W{self.layer_idx}&#39;]
        b = params[f&#39;B{self.layer_idx}&#39;]        
        prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]        
        
        # Referenced object `params` is modified
        np.dot(w, prev_a)
        params[f&#39;Z{self.layer_idx}&#39;] = np.dot(w, prev_a) + b        
        params[f&#39;A{self.layer_idx}&#39;] = self.activation(params[f&#39;Z{self.layer_idx}&#39;])

    def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
        &#34;&#34;&#34;Performs backpropagation by calculating weight and bias gradients and returning `gradient` which is dL/dx.

        Args:
            gradient: Partial derivative with respect to activations from previous layer.
            is_last (bool, optional): Whether this layer is the last one in the model. Defaults to False.
            output (optional): Output from this layer during forward pass. Defaults to None.
            target (optional): The target output from the model. Only used when `is_last` is True. Defaults to None.
            new_params (dict, optional): Dictionary with model gradients for weights and biases. Defaults to None.
            model (optional): Reference to the `Sequential` model. Defaults to None.

        Returns:
            np.array: dL/dx. How the loss-function is affected by input changes.
        &#34;&#34;&#34;

        activation_deriv = model.layers[self.layer_idx].activation(model.params[f&#39;Z{self.layer_idx}&#39;], derivative=True)        
        if not is_last:            
            local = gradient * activation_deriv
        else:           
            local = 2 * (output - target) / output.shape[0] * activation_deriv
        
        new_params[f&#39;W{self.layer_idx}&#39;] = np.outer(local, model.params[f&#39;A{self.layer_idx - 1}&#39;])                                
        new_params[f&#39;B{self.layer_idx}&#39;] = local                

        out = np.dot(model.params[f&#39;W{self.layer_idx}&#39;].T, local.T)        
        return out

    def __str__(self) -&gt; str:
        return f&#34;&lt;Dense layer class object named &#39;{self.name}&#39; of size {self.size} using activation function &#39;{self.activation.__name__}&#39;&gt;&#34;</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pysnaike.layers.Dense.backward_pass"><code class="name flex">
<span>def <span class="ident">backward_pass</span></span>(<span>self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs backpropagation by calculating weight and bias gradients and returning <code>gradient</code> which is dL/dx.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gradient</code></strong></dt>
<dd>Partial derivative with respect to activations from previous layer.</dd>
<dt><strong><code>is_last</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether this layer is the last one in the model. Defaults to False.</dd>
<dt><strong><code>output</code></strong> :&ensp;<code>optional</code></dt>
<dd>Output from this layer during forward pass. Defaults to None.</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>optional</code></dt>
<dd>The target output from the model. Only used when <code>is_last</code> is True. Defaults to None.</dd>
<dt><strong><code>new_params</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Dictionary with model gradients for weights and biases. Defaults to None.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>optional</code></dt>
<dd>Reference to the <code>Sequential</code> model. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>dL/dx. How the loss-function is affected by input changes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
    &#34;&#34;&#34;Performs backpropagation by calculating weight and bias gradients and returning `gradient` which is dL/dx.

    Args:
        gradient: Partial derivative with respect to activations from previous layer.
        is_last (bool, optional): Whether this layer is the last one in the model. Defaults to False.
        output (optional): Output from this layer during forward pass. Defaults to None.
        target (optional): The target output from the model. Only used when `is_last` is True. Defaults to None.
        new_params (dict, optional): Dictionary with model gradients for weights and biases. Defaults to None.
        model (optional): Reference to the `Sequential` model. Defaults to None.

    Returns:
        np.array: dL/dx. How the loss-function is affected by input changes.
    &#34;&#34;&#34;

    activation_deriv = model.layers[self.layer_idx].activation(model.params[f&#39;Z{self.layer_idx}&#39;], derivative=True)        
    if not is_last:            
        local = gradient * activation_deriv
    else:           
        local = 2 * (output - target) / output.shape[0] * activation_deriv
    
    new_params[f&#39;W{self.layer_idx}&#39;] = np.outer(local, model.params[f&#39;A{self.layer_idx - 1}&#39;])                                
    new_params[f&#39;B{self.layer_idx}&#39;] = local                

    out = np.dot(model.params[f&#39;W{self.layer_idx}&#39;].T, local.T)        
    return out</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.Dense.forward_pass"><code class="name flex">
<span>def <span class="ident">forward_pass</span></span>(<span>self, params)</span>
</code></dt>
<dd>
<div class="desc"><p>Forward pass through dense layer.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>params</code></strong> :&ensp;<code>dict</code></dt>
<dd>Model parameters.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_pass(self, params):
    &#34;&#34;&#34;Forward pass through dense layer.

    Args:
        params (dict): Model parameters.
    &#34;&#34;&#34;
    w = params[f&#39;W{self.layer_idx}&#39;]
    b = params[f&#39;B{self.layer_idx}&#39;]        
    prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]        
    
    # Referenced object `params` is modified
    np.dot(w, prev_a)
    params[f&#39;Z{self.layer_idx}&#39;] = np.dot(w, prev_a) + b        
    params[f&#39;A{self.layer_idx}&#39;] = self.activation(params[f&#39;Z{self.layer_idx}&#39;])</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.Dense.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Called on model compile.</p>
<h2 id="returns">Returns</h2>
<p>(w, b): Weight and bias.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, **kvargs):
    &#34;&#34;&#34;Called on model compile.

    Returns:
        (w, b): Weight and bias.
    &#34;&#34;&#34;

    self.layer_idx = kvargs[&#39;layer_idx&#39;]               
    w = np.random.randn(kvargs[&#39;size_curr&#39;], np.prod(kvargs[&#39;size_prev&#39;])) * 0.1
    b = np.zeros(kvargs[&#39;size_curr&#39;])
    return w, b</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pysnaike.layers.Flatten"><code class="flex name class">
<span>class <span class="ident">Flatten</span></span>
<span>(</span><span>name='flatten')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Flatten():
    def __init__(self, name=&#39;flatten&#39;):
        self.name = name
        self.output_shape = None
        self.num_params = None
        self.size_prev = None

    def setup(self, **kvargs):
        self.layer_idx = kvargs[&#39;layer_idx&#39;]
        self.size_prev = kvargs[&#39;size_prev&#39;]
        self.output_shape = np.array([np.prod(self.size_prev)])
        return None, None

    def forward_pass(self, params):        
        params[f&#39;A{self.layer_idx}&#39;] = params[f&#39;A{self.layer_idx - 1}&#39;].flatten()

    def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
        if not is_last:
            local = gradient
        else:
            local = 2 * (output - target)

        return local.reshape(self.size_prev)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pysnaike.layers.Flatten.backward_pass"><code class="name flex">
<span>def <span class="ident">backward_pass</span></span>(<span>self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
    if not is_last:
        local = gradient
    else:
        local = 2 * (output - target)

    return local.reshape(self.size_prev)</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.Flatten.forward_pass"><code class="name flex">
<span>def <span class="ident">forward_pass</span></span>(<span>self, params)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_pass(self, params):        
    params[f&#39;A{self.layer_idx}&#39;] = params[f&#39;A{self.layer_idx - 1}&#39;].flatten()</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.Flatten.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, **kvargs):
    self.layer_idx = kvargs[&#39;layer_idx&#39;]
    self.size_prev = kvargs[&#39;size_prev&#39;]
    self.output_shape = np.array([np.prod(self.size_prev)])
    return None, None</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pysnaike.layers.MaxPooling2D"><code class="flex name class">
<span>class <span class="ident">MaxPooling2D</span></span>
<span>(</span><span>pool_size, name='max_pool')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class MaxPooling2D():
    def __init__(self, pool_size, name=&#34;max_pool&#34;):
        self.pool_size = np.array(pool_size)
        self.name = name
        self.output_shape = None
        self.num_params = None

    def setup(self, **kvargs):
        self.layer_idx = kvargs[&#39;layer_idx&#39;]
        self.input_shape = kvargs[&#39;size_prev&#39;]
        self.output_shape = (self.input_shape[0], *(self.input_shape[-2:] // self.pool_size))

        # Updating pool size with prev layer channel dimension
        self.pool_size = np.array((self.input_shape[0], *self.pool_size))
        idx = np.ones(self.input_shape)
        return idx, None

    def pool(self, a, b, out_shape):
        &#34;&#34;&#34;Convolve `b` across `a`.

        Args:
            a: Stationary matrix.
            b: Moving pool matrix shape.

        Todo:
            Use numpy slide and stride. [::]
        &#34;&#34;&#34;
        
        stride = b[-2:]

        out = np.zeros(out_shape)
        out_coords = np.zeros((*out.shape, 2))

        for x in range(out.shape[-2]):
            for y in range(out.shape[-1]):
                arr_x = x * stride[0]
                arr_y = y * stride[1]

                curr_a = a[:, arr_x : arr_x + stride[0], arr_y : arr_y + stride[1]]
                curr_a_reshape = curr_a.reshape(curr_a.shape[0], -1)
                max = curr_a_reshape.argmax(axis=1)
                coords = np.column_stack(np.unravel_index(max, curr_a.shape[1:]))
                vals = curr_a_reshape[np.arange(curr_a_reshape.shape[0]), max]

                coords = coords.T
                coords[0] += arr_x
                coords[1] += arr_y
                out_coords[:,x,y,:] = coords.T
                out[:,x,y] = vals

        return out_coords, out


    def forward_pass(self, params):

        prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]
        out_coords, out = self.pool(prev_a, self.pool_size, self.output_shape)        
        for channel in range(out.shape[0]):
            max = np.max(out[channel])           
            min = np.min(out[channel])      
            divisor = np.max([max, np.absolute(min)])
            out[channel] /= divisor            
        
        params[f&#39;A{self.layer_idx}&#39;] = out
        params[f&#39;I{self.layer_idx}&#39;] = out_coords.astype(int)
        
    def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
        &#34;&#34;&#34;Performs backpropagation by calculating weight and bias gradients and returning `gradient` which is dL/dx.

        Args:
            gradient: Partial derivative with respect to activations from previous layer.
            is_last (bool, optional): Whether this layer is the last one in the model. Defaults to False.
            output (optional): Output from this layer during forward pass. Defaults to None.
            target (optional): The target output from the model. Only used when `is_last` is True. Defaults to None.
            new_params (dict, optional): Dictionary with model gradients for weights and biases. Defaults to None.
            model (optional): Reference to the `Sequential` model. Defaults to None.

        Returns:
            np.array: dL/dx. How the loss-function is affected by input changes.
        &#34;&#34;&#34;

        if not is_last:
            local = gradient
        else:
            local = 2 * (output - target)
        
        arr_idx = model.params[&#39;I&#39; + str(self.layer_idx)].copy()
        out_gradient = np.ones(self.input_shape)
        for n in range(arr_idx.shape[0]):
            n_coords = np.concatenate(arr_idx[n], 0)
            n_coords = n_coords.T
            out_gradient[n, n_coords[0], n_coords[1]] = local[n].flatten()

        return out_gradient</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pysnaike.layers.MaxPooling2D.backward_pass"><code class="name flex">
<span>def <span class="ident">backward_pass</span></span>(<span>self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Performs backpropagation by calculating weight and bias gradients and returning <code>gradient</code> which is dL/dx.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>gradient</code></strong></dt>
<dd>Partial derivative with respect to activations from previous layer.</dd>
<dt><strong><code>is_last</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Whether this layer is the last one in the model. Defaults to False.</dd>
<dt><strong><code>output</code></strong> :&ensp;<code>optional</code></dt>
<dd>Output from this layer during forward pass. Defaults to None.</dd>
<dt><strong><code>target</code></strong> :&ensp;<code>optional</code></dt>
<dd>The target output from the model. Only used when <code>is_last</code> is True. Defaults to None.</dd>
<dt><strong><code>new_params</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Dictionary with model gradients for weights and biases. Defaults to None.</dd>
<dt><strong><code>model</code></strong> :&ensp;<code>optional</code></dt>
<dd>Reference to the <code>Sequential</code> model. Defaults to None.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>np.array</code></dt>
<dd>dL/dx. How the loss-function is affected by input changes.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
    &#34;&#34;&#34;Performs backpropagation by calculating weight and bias gradients and returning `gradient` which is dL/dx.

    Args:
        gradient: Partial derivative with respect to activations from previous layer.
        is_last (bool, optional): Whether this layer is the last one in the model. Defaults to False.
        output (optional): Output from this layer during forward pass. Defaults to None.
        target (optional): The target output from the model. Only used when `is_last` is True. Defaults to None.
        new_params (dict, optional): Dictionary with model gradients for weights and biases. Defaults to None.
        model (optional): Reference to the `Sequential` model. Defaults to None.

    Returns:
        np.array: dL/dx. How the loss-function is affected by input changes.
    &#34;&#34;&#34;

    if not is_last:
        local = gradient
    else:
        local = 2 * (output - target)
    
    arr_idx = model.params[&#39;I&#39; + str(self.layer_idx)].copy()
    out_gradient = np.ones(self.input_shape)
    for n in range(arr_idx.shape[0]):
        n_coords = np.concatenate(arr_idx[n], 0)
        n_coords = n_coords.T
        out_gradient[n, n_coords[0], n_coords[1]] = local[n].flatten()

    return out_gradient</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.MaxPooling2D.forward_pass"><code class="name flex">
<span>def <span class="ident">forward_pass</span></span>(<span>self, params)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_pass(self, params):

    prev_a = params[f&#39;A{self.layer_idx - 1}&#39;]
    out_coords, out = self.pool(prev_a, self.pool_size, self.output_shape)        
    for channel in range(out.shape[0]):
        max = np.max(out[channel])           
        min = np.min(out[channel])      
        divisor = np.max([max, np.absolute(min)])
        out[channel] /= divisor            
    
    params[f&#39;A{self.layer_idx}&#39;] = out
    params[f&#39;I{self.layer_idx}&#39;] = out_coords.astype(int)</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.MaxPooling2D.pool"><code class="name flex">
<span>def <span class="ident">pool</span></span>(<span>self, a, b, out_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Convolve <code>b</code> across <code>a</code>.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>a</code></strong></dt>
<dd>Stationary matrix.</dd>
<dt><strong><code>b</code></strong></dt>
<dd>Moving pool matrix shape.</dd>
</dl>
<h2 id="todo">Todo</h2>
<p>Use numpy slide and stride. [::]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pool(self, a, b, out_shape):
    &#34;&#34;&#34;Convolve `b` across `a`.

    Args:
        a: Stationary matrix.
        b: Moving pool matrix shape.

    Todo:
        Use numpy slide and stride. [::]
    &#34;&#34;&#34;
    
    stride = b[-2:]

    out = np.zeros(out_shape)
    out_coords = np.zeros((*out.shape, 2))

    for x in range(out.shape[-2]):
        for y in range(out.shape[-1]):
            arr_x = x * stride[0]
            arr_y = y * stride[1]

            curr_a = a[:, arr_x : arr_x + stride[0], arr_y : arr_y + stride[1]]
            curr_a_reshape = curr_a.reshape(curr_a.shape[0], -1)
            max = curr_a_reshape.argmax(axis=1)
            coords = np.column_stack(np.unravel_index(max, curr_a.shape[1:]))
            vals = curr_a_reshape[np.arange(curr_a_reshape.shape[0]), max]

            coords = coords.T
            coords[0] += arr_x
            coords[1] += arr_y
            out_coords[:,x,y,:] = coords.T
            out[:,x,y] = vals

    return out_coords, out</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.MaxPooling2D.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, **kvargs):
    self.layer_idx = kvargs[&#39;layer_idx&#39;]
    self.input_shape = kvargs[&#39;size_prev&#39;]
    self.output_shape = (self.input_shape[0], *(self.input_shape[-2:] // self.pool_size))

    # Updating pool size with prev layer channel dimension
    self.pool_size = np.array((self.input_shape[0], *self.pool_size))
    idx = np.ones(self.input_shape)
    return idx, None</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="pysnaike.layers.Reshape"><code class="flex name class">
<span>class <span class="ident">Reshape</span></span>
<span>(</span><span>output_shape, name='reshape')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Reshape():
    def __init__(self, output_shape, name=&#39;reshape&#39;):
        self.name = name
        self.input_shape = np.array([0])
        self.output_shape = output_shape
        self.num_params = None
        self.size_prev = None        

    def setup(self, **kvargs):
        self.layer_idx = kvargs[&#39;layer_idx&#39;]
        self.size_prev = kvargs[&#39;size_prev&#39;]
        return None, None

    def forward_pass(self, params):        
        params[f&#39;A{self.layer_idx}&#39;] = params[f&#39;A{self.layer_idx - 1}&#39;].reshape(self.output_shape)
        
    def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
        if not is_last:
            local = gradient
        else:
            local = 2 * (output - target)

        return local.reshape(self.size_prev)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="pysnaike.layers.Reshape.backward_pass"><code class="name flex">
<span>def <span class="ident">backward_pass</span></span>(<span>self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backward_pass(self, gradient, is_last=False, output=None, target=None, new_params=None, model=None, **kvargs):
    if not is_last:
        local = gradient
    else:
        local = 2 * (output - target)

    return local.reshape(self.size_prev)</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.Reshape.forward_pass"><code class="name flex">
<span>def <span class="ident">forward_pass</span></span>(<span>self, params)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward_pass(self, params):        
    params[f&#39;A{self.layer_idx}&#39;] = params[f&#39;A{self.layer_idx - 1}&#39;].reshape(self.output_shape)</code></pre>
</details>
</dd>
<dt id="pysnaike.layers.Reshape.setup"><code class="name flex">
<span>def <span class="ident">setup</span></span>(<span>self, **kvargs)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def setup(self, **kvargs):
    self.layer_idx = kvargs[&#39;layer_idx&#39;]
    self.size_prev = kvargs[&#39;size_prev&#39;]
    return None, None</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="pysnaike" href="index.html">pysnaike</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="pysnaike.layers.AvgPooling2D" href="#pysnaike.layers.AvgPooling2D">AvgPooling2D</a></code></h4>
</li>
<li>
<h4><code><a title="pysnaike.layers.Conv2D" href="#pysnaike.layers.Conv2D">Conv2D</a></code></h4>
<ul class="two-column">
<li><code><a title="pysnaike.layers.Conv2D.backward_pass" href="#pysnaike.layers.Conv2D.backward_pass">backward_pass</a></code></li>
<li><code><a title="pysnaike.layers.Conv2D.calc_num_params" href="#pysnaike.layers.Conv2D.calc_num_params">calc_num_params</a></code></li>
<li><code><a title="pysnaike.layers.Conv2D.calc_output_shape" href="#pysnaike.layers.Conv2D.calc_output_shape">calc_output_shape</a></code></li>
<li><code><a title="pysnaike.layers.Conv2D.calc_padding" href="#pysnaike.layers.Conv2D.calc_padding">calc_padding</a></code></li>
<li><code><a title="pysnaike.layers.Conv2D.convolve" href="#pysnaike.layers.Conv2D.convolve">convolve</a></code></li>
<li><code><a title="pysnaike.layers.Conv2D.forward_pass" href="#pysnaike.layers.Conv2D.forward_pass">forward_pass</a></code></li>
<li><code><a title="pysnaike.layers.Conv2D.full_back_conv" href="#pysnaike.layers.Conv2D.full_back_conv">full_back_conv</a></code></li>
<li><code><a title="pysnaike.layers.Conv2D.setup" href="#pysnaike.layers.Conv2D.setup">setup</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pysnaike.layers.Dense" href="#pysnaike.layers.Dense">Dense</a></code></h4>
<ul class="">
<li><code><a title="pysnaike.layers.Dense.backward_pass" href="#pysnaike.layers.Dense.backward_pass">backward_pass</a></code></li>
<li><code><a title="pysnaike.layers.Dense.forward_pass" href="#pysnaike.layers.Dense.forward_pass">forward_pass</a></code></li>
<li><code><a title="pysnaike.layers.Dense.setup" href="#pysnaike.layers.Dense.setup">setup</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pysnaike.layers.Flatten" href="#pysnaike.layers.Flatten">Flatten</a></code></h4>
<ul class="">
<li><code><a title="pysnaike.layers.Flatten.backward_pass" href="#pysnaike.layers.Flatten.backward_pass">backward_pass</a></code></li>
<li><code><a title="pysnaike.layers.Flatten.forward_pass" href="#pysnaike.layers.Flatten.forward_pass">forward_pass</a></code></li>
<li><code><a title="pysnaike.layers.Flatten.setup" href="#pysnaike.layers.Flatten.setup">setup</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pysnaike.layers.MaxPooling2D" href="#pysnaike.layers.MaxPooling2D">MaxPooling2D</a></code></h4>
<ul class="">
<li><code><a title="pysnaike.layers.MaxPooling2D.backward_pass" href="#pysnaike.layers.MaxPooling2D.backward_pass">backward_pass</a></code></li>
<li><code><a title="pysnaike.layers.MaxPooling2D.forward_pass" href="#pysnaike.layers.MaxPooling2D.forward_pass">forward_pass</a></code></li>
<li><code><a title="pysnaike.layers.MaxPooling2D.pool" href="#pysnaike.layers.MaxPooling2D.pool">pool</a></code></li>
<li><code><a title="pysnaike.layers.MaxPooling2D.setup" href="#pysnaike.layers.MaxPooling2D.setup">setup</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="pysnaike.layers.Reshape" href="#pysnaike.layers.Reshape">Reshape</a></code></h4>
<ul class="">
<li><code><a title="pysnaike.layers.Reshape.backward_pass" href="#pysnaike.layers.Reshape.backward_pass">backward_pass</a></code></li>
<li><code><a title="pysnaike.layers.Reshape.forward_pass" href="#pysnaike.layers.Reshape.forward_pass">forward_pass</a></code></li>
<li><code><a title="pysnaike.layers.Reshape.setup" href="#pysnaike.layers.Reshape.setup">setup</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>